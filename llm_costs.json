[
  {
    "model": "ai21.jamba-1-5-large-v1:0",
    "provider": "bedrock",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "ai21.jamba-1-5-mini-v1:0",
    "provider": "bedrock",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "us.writer.palmyra-x4-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "us.writer.palmyra-x5-v1:0",
    "provider": "bedrock_converse",
    "input": 6E-7,
    "output": 0.000006,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "writer.palmyra-x4-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "writer.palmyra-x5-v1:0",
    "provider": "bedrock_converse",
    "input": 6E-7,
    "output": 0.000006,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "amazon.nova-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 6E-8,
    "output": 2.4E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "amazon.nova-2-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "amazon.nova-2-pro-preview-20251202-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000021875,
    "output": 0.0000175,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "apac.amazon.nova-2-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 3.3E-7,
    "output": 0.00000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "apac.amazon.nova-2-pro-preview-20251202-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000021875,
    "output": 0.0000175,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.amazon.nova-2-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 3.3E-7,
    "output": 0.00000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.amazon.nova-2-pro-preview-20251202-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000021875,
    "output": 0.0000175,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.amazon.nova-2-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 3.3E-7,
    "output": 0.00000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.amazon.nova-2-pro-preview-20251202-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000021875,
    "output": 0.0000175,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "amazon.nova-micro-v1:0",
    "provider": "bedrock_converse",
    "input": 3.5E-8,
    "output": 1.4E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 10000
  },
  {
    "model": "amazon.nova-pro-v1:0",
    "provider": "bedrock_converse",
    "input": 8E-7,
    "output": 0.0000032,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.000004,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-haiku-4-5@20251001",
    "provider": "bedrock_converse",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 4096
  },
  {
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "anthropic.claude-3-7-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.0000036,
    "output": 0.000018,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "anthropic.claude-3-7-sonnet-20250219-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "anthropic.claude-3-opus-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "anthropic.claude-instant-v1",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.0000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "anthropic.claude-opus-4-1-20250805-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "anthropic.claude-opus-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "anthropic.claude-opus-4-5-20251101-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-opus-4-6-v1",
    "provider": "bedrock_converse",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "global.anthropic.claude-opus-4-6-v1",
    "provider": "bedrock_converse",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "us.anthropic.claude-opus-4-6-v1",
    "provider": "bedrock_converse",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "eu.anthropic.claude-opus-4-6-v1",
    "provider": "bedrock_converse",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "au.anthropic.claude-opus-4-6-v1",
    "provider": "bedrock_converse",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "anthropic.claude-sonnet-4-6",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "global.anthropic.claude-sonnet-4-6",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.anthropic.claude-sonnet-4-6",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.anthropic.claude-sonnet-4-6",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "apac.anthropic.claude-sonnet-4-6",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-sonnet-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "anthropic.claude-v1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "anthropic.claude-v2:1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "apac.amazon.nova-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 6.3E-8,
    "output": 2.52E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "apac.amazon.nova-micro-v1:0",
    "provider": "bedrock_converse",
    "input": 3.7E-8,
    "output": 1.48E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 10000
  },
  {
    "model": "apac.amazon.nova-pro-v1:0",
    "provider": "bedrock_converse",
    "input": 8.4E-7,
    "output": 0.00000336,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "apac.anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "apac.anthropic.claude-3-5-sonnet-20241022-v2:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "apac.anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "apac.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000011,
    "output": 0.0000055,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "apac.anthropic.claude-3-sonnet-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "apac.anthropic.claude-sonnet-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "au.anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "azure/codex-mini",
    "provider": "azure",
    "input": 0.0000015,
    "output": 0.000006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/command-r-plus",
    "provider": "azure",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/claude-haiku-4-5",
    "provider": "azure_ai",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "azure_ai/claude-opus-4-5",
    "provider": "azure_ai",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "azure_ai/claude-opus-4-6",
    "provider": "azure_ai",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure_ai/claude-opus-4-1",
    "provider": "azure_ai",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "azure_ai/claude-sonnet-4-5",
    "provider": "azure_ai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "azure_ai/claude-sonnet-4-6",
    "provider": "azure_ai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "azure_ai/gpt-oss-120b",
    "provider": "azure_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure/eu/gpt-4o-2024-08-06",
    "provider": "azure",
    "input": 0.00000275,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/eu/gpt-4o-2024-11-20",
    "provider": "azure",
    "input": 0.00000275,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/eu/gpt-4o-mini-2024-07-18",
    "provider": "azure",
    "input": 1.65E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/eu/gpt-4o-mini-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 6.6E-7,
    "output": 0.00000264,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/eu/gpt-4o-realtime-preview-2024-10-01",
    "provider": "azure",
    "input": 0.0000055,
    "output": 0.000022,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/eu/gpt-4o-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 0.0000055,
    "output": 0.000022,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/eu/gpt-5-2025-08-07",
    "provider": "azure",
    "input": 0.000001375,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5-mini-2025-08-07",
    "provider": "azure",
    "input": 2.75E-7,
    "output": 0.0000022,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5.1",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5.1-chat",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5.1-codex",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5.1-codex-mini",
    "provider": "azure",
    "input": 2.75E-7,
    "output": 0.0000022,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/gpt-5-nano-2025-08-07",
    "provider": "azure",
    "input": 5.5E-8,
    "output": 4.4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/eu/o1-2024-12-17",
    "provider": "azure",
    "input": 0.0000165,
    "output": 0.000066,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/eu/o1-mini-2024-09-12",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "azure/eu/o1-preview-2024-09-12",
    "provider": "azure",
    "input": 0.0000165,
    "output": 0.000066,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/eu/o3-mini-2025-01-31",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/global-standard/gpt-4o-2024-08-06",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/global-standard/gpt-4o-2024-11-20",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/global-standard/gpt-4o-mini",
    "provider": "azure",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/global/gpt-4o-2024-08-06",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/global/gpt-4o-2024-11-20",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/global/gpt-5.1",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/global/gpt-5.1-chat",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/global/gpt-5.1-codex",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/global/gpt-5.1-codex-mini",
    "provider": "azure",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-4-0125-preview",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4-1106-preview",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4-turbo",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4-turbo-2024-04-09",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4-turbo-vision-preview",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4.1",
    "provider": "azure",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.1-2025-04-14",
    "provider": "azure",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.1-mini",
    "provider": "azure",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.1-mini-2025-04-14",
    "provider": "azure",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.1-nano",
    "provider": "azure",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.1-nano-2025-04-14",
    "provider": "azure",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/gpt-4.5-preview",
    "provider": "azure",
    "input": 0.000075,
    "output": 0.00015,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-2024-05-13",
    "provider": "azure",
    "input": 0.000005,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4o-2024-08-06",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-2024-11-20",
    "provider": "azure",
    "input": 0.00000275,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-audio-2025-08-28",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-audio-mini-2025-10-06",
    "provider": "azure",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-audio-preview-2024-12-17",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-mini",
    "provider": "azure",
    "input": 1.65E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-mini-2024-07-18",
    "provider": "azure",
    "input": 1.65E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-mini-audio-preview-2024-12-17",
    "provider": "azure",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-4o-mini-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4o-realtime-preview-2024-10-01",
    "provider": "azure",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-4o-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/gpt-5.1-2025-11-13",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-chat-2025-11-13",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-5.1-codex-2025-11-13",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-codex-mini-2025-11-13",
    "provider": "azure",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-2025-08-07",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-chat",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-5-chat-latest",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-5-codex",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-mini",
    "provider": "azure",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-mini-2025-08-07",
    "provider": "azure",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-nano",
    "provider": "azure",
    "input": 5E-8,
    "output": 4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-nano-2025-08-07",
    "provider": "azure",
    "input": 5E-8,
    "output": 4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5-pro",
    "provider": "azure",
    "input": 0.000015,
    "output": 0.00012,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-chat",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-codex",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-codex-max",
    "provider": "azure",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.1-codex-mini",
    "provider": "azure",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.2",
    "provider": "azure",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.2-2025-12-11",
    "provider": "azure",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.2-chat",
    "provider": "azure",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-5.2-chat-2025-12-11",
    "provider": "azure",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/gpt-5.2-codex",
    "provider": "azure",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.2-pro",
    "provider": "azure",
    "input": 0.000021,
    "output": 0.000168,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/gpt-5.2-pro-2025-12-11",
    "provider": "azure",
    "input": 0.000021,
    "output": 0.000168,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/o1",
    "provider": "azure",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o1-2024-12-17",
    "provider": "azure",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o1-mini",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "azure/o1-mini-2024-09-12",
    "provider": "azure",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "azure/o1-preview",
    "provider": "azure",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/o1-preview-2024-09-12",
    "provider": "azure",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/o3",
    "provider": "azure",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-2025-04-16",
    "provider": "azure",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-deep-research",
    "provider": "azure",
    "input": 0.00001,
    "output": 0.00004,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-mini",
    "provider": "azure",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-mini-2025-01-31",
    "provider": "azure",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-pro",
    "provider": "azure",
    "input": 0.00002,
    "output": 0.00008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o3-pro-2025-06-10",
    "provider": "azure",
    "input": 0.00002,
    "output": 0.00008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o4-mini",
    "provider": "azure",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/o4-mini-2025-04-16",
    "provider": "azure",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/us/gpt-4.1-2025-04-14",
    "provider": "azure",
    "input": 0.0000022,
    "output": 0.0000088,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/us/gpt-4.1-mini-2025-04-14",
    "provider": "azure",
    "input": 4.4E-7,
    "output": 0.00000176,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/us/gpt-4.1-nano-2025-04-14",
    "provider": "azure",
    "input": 1.1E-7,
    "output": 4.4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/us/gpt-4o-2024-08-06",
    "provider": "azure",
    "input": 0.00000275,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/us/gpt-4o-2024-11-20",
    "provider": "azure",
    "input": 0.00000275,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/us/gpt-4o-mini-2024-07-18",
    "provider": "azure",
    "input": 1.65E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure/us/gpt-4o-mini-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 6.6E-7,
    "output": 0.00000264,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/us/gpt-4o-realtime-preview-2024-10-01",
    "provider": "azure",
    "input": 0.0000055,
    "output": 0.000022,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/us/gpt-4o-realtime-preview-2024-12-17",
    "provider": "azure",
    "input": 0.0000055,
    "output": 0.000022,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure/us/gpt-5-2025-08-07",
    "provider": "azure",
    "input": 0.000001375,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5-mini-2025-08-07",
    "provider": "azure",
    "input": 2.75E-7,
    "output": 0.0000022,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5-nano-2025-08-07",
    "provider": "azure",
    "input": 5.5E-8,
    "output": 4.4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5.1",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5.1-chat",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5.1-codex",
    "provider": "azure",
    "input": 0.00000138,
    "output": 0.000011,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/gpt-5.1-codex-mini",
    "provider": "azure",
    "input": 2.75E-7,
    "output": 0.0000022,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "azure/us/o1-2024-12-17",
    "provider": "azure",
    "input": 0.0000165,
    "output": 0.000066,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/us/o1-mini-2024-09-12",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "azure/us/o1-preview-2024-09-12",
    "provider": "azure",
    "input": 0.0000165,
    "output": 0.000066,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "azure/us/o3-2025-04-16",
    "provider": "azure",
    "input": 0.0000022,
    "output": 0.0000088,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/us/o3-mini-2025-01-31",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure/us/o4-mini-2025-04-16",
    "provider": "azure",
    "input": 0.00000121,
    "output": 0.00000484,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "azure_ai/Llama-3.2-11B-Vision-Instruct",
    "provider": "azure_ai",
    "input": 3.7E-7,
    "output": 3.7E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Llama-3.2-90B-Vision-Instruct",
    "provider": "azure_ai",
    "input": 0.00000204,
    "output": 0.00000204,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Llama-3.3-70B-Instruct",
    "provider": "azure_ai",
    "input": 7.1E-7,
    "output": 7.1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "provider": "azure_ai",
    "input": 0.00000141,
    "output": 3.5E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure_ai/Llama-4-Scout-17B-16E-Instruct",
    "provider": "azure_ai",
    "input": 2E-7,
    "output": 7.8E-7,
    "max_input_tokens": 10000000,
    "max_output_tokens": 16384
  },
  {
    "model": "azure_ai/Meta-Llama-3.1-405B-Instruct",
    "provider": "azure_ai",
    "input": 0.00000533,
    "output": 0.000016,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Meta-Llama-3.1-70B-Instruct",
    "provider": "azure_ai",
    "input": 0.00000268,
    "output": 0.00000354,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Meta-Llama-3.1-8B-Instruct",
    "provider": "azure_ai",
    "input": 3E-7,
    "output": 6.1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "azure_ai/Phi-3-medium-128k-instruct",
    "provider": "azure_ai",
    "input": 1.7E-7,
    "output": 6.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-3-mini-128k-instruct",
    "provider": "azure_ai",
    "input": 1.3E-7,
    "output": 5.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-3-small-128k-instruct",
    "provider": "azure_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-3.5-MoE-instruct",
    "provider": "azure_ai",
    "input": 1.6E-7,
    "output": 6.4E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-3.5-mini-instruct",
    "provider": "azure_ai",
    "input": 1.3E-7,
    "output": 5.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-3.5-vision-instruct",
    "provider": "azure_ai",
    "input": 1.3E-7,
    "output": 5.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-4-mini-instruct",
    "provider": "azure_ai",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-4-multimodal-instruct",
    "provider": "azure_ai",
    "input": 8E-8,
    "output": 3.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/Phi-4-mini-reasoning",
    "provider": "azure_ai",
    "input": 8E-8,
    "output": 3.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/MAI-DS-R1",
    "provider": "azure_ai",
    "input": 0.00000135,
    "output": 0.0000054,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "azure_ai/deepseek-v3.2",
    "provider": "azure_ai",
    "input": 5.8E-7,
    "output": 0.00000168,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "azure_ai/deepseek-v3.2-speciale",
    "provider": "azure_ai",
    "input": 5.8E-7,
    "output": 0.00000168,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "azure_ai/deepseek-r1",
    "provider": "azure_ai",
    "input": 0.00000135,
    "output": 0.0000054,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "azure_ai/deepseek-v3",
    "provider": "azure_ai",
    "input": 0.00000114,
    "output": 0.00000456,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "azure_ai/deepseek-v3-0324",
    "provider": "azure_ai",
    "input": 0.00000114,
    "output": 0.00000456,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "azure_ai/embed-v-4-0",
    "provider": "azure_ai",
    "input": 1.2E-7,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "azure_ai/global/grok-3",
    "provider": "azure_ai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/global/grok-3-mini",
    "provider": "azure_ai",
    "input": 2.5E-7,
    "output": 0.00000127,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-3",
    "provider": "azure_ai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-3-mini",
    "provider": "azure_ai",
    "input": 2.5E-7,
    "output": 0.00000127,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-4",
    "provider": "azure_ai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-4-fast-non-reasoning",
    "provider": "azure_ai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-4-fast-reasoning",
    "provider": "azure_ai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/grok-code-fast-1",
    "provider": "azure_ai",
    "input": 2E-7,
    "output": 0.0000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "azure_ai/kimi-k2.5",
    "provider": "azure_ai",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "azure_ai/ministral-3b",
    "provider": "azure_ai",
    "input": 4E-8,
    "output": 4E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/mistral-large-2407",
    "provider": "azure_ai",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/mistral-large-latest",
    "provider": "azure_ai",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/mistral-large-3",
    "provider": "azure_ai",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 8191
  },
  {
    "model": "azure_ai/mistral-medium-2505",
    "provider": "azure_ai",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 8191
  },
  {
    "model": "azure_ai/mistral-nemo",
    "provider": "azure_ai",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 4096
  },
  {
    "model": "azure_ai/mistral-small-2503",
    "provider": "azure_ai",
    "input": 0.000001,
    "output": 0.000003,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "bedrock/ap-northeast-1/anthropic.claude-instant-v1",
    "provider": "bedrock",
    "input": 0.00000223,
    "output": 0.00000755,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/ap-northeast-1/anthropic.claude-v1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/ap-northeast-1/anthropic.claude-v2:1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/ap-northeast-1/deepseek.v3.2",
    "provider": "bedrock",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/ap-northeast-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/ap-northeast-1/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 7.3E-7,
    "output": 0.00000303,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-northeast-1/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 7.2E-7,
    "output": 0.0000036,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-northeast-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 7.3E-7,
    "output": 0.00000303,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000303,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-south-1/deepseek.v3.2",
    "provider": "bedrock",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/ap-south-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/ap-south-1/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 7.1E-7,
    "output": 0.00000294,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-south-1/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 7.2E-7,
    "output": 0.0000036,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-south-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/ap-southeast-3/deepseek.v3.2",
    "provider": "bedrock",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/ap-southeast-3/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/ap-southeast-3/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 7.2E-7,
    "output": 0.0000036,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/ap-southeast-3/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-north-1/deepseek.v3.2",
    "provider": "bedrock",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/eu-north-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-north-1/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 7.2E-7,
    "output": 0.0000036,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/eu-central-1/anthropic.claude-instant-v1",
    "provider": "bedrock",
    "input": 0.00000248,
    "output": 0.00000838,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/eu-central-1/anthropic.claude-v1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/eu-central-1/anthropic.claude-v2:1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/eu-central-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-central-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-west-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-west-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-west-2/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 4.7E-7,
    "output": 0.00000186,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-west-2/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 7.8E-7,
    "output": 0.00000186,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-south-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/eu-south-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "bedrock/sa-east-1/deepseek.v3.2",
    "provider": "bedrock",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/sa-east-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3.6E-7,
    "output": 0.00000144,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/sa-east-1/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 7.3E-7,
    "output": 0.00000303,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/sa-east-1/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 7.2E-7,
    "output": 0.0000036,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/sa-east-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.00000144,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-east-1/anthropic.claude-instant-v1",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.0000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-east-1/anthropic.claude-v1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-east-1/anthropic.claude-v2:1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-east-1/deepseek.v3.2",
    "provider": "bedrock",
    "input": 6.2E-7,
    "output": 0.00000185,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/us-east-1/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-east-1/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-east-1/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-east-1/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-east-2/deepseek.v3.2",
    "provider": "bedrock",
    "input": 6.2E-7,
    "output": 0.00000185,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/us-east-2/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-east-2/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-east-2/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-east-2/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-gov-east-1/amazon.nova-pro-v1:0",
    "provider": "bedrock",
    "input": 9.6E-7,
    "output": 0.00000384,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "bedrock/us-gov-east-1/anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.0000036,
    "output": 0.000018,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-gov-east-1/anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 3E-7,
    "output": 0.0000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "bedrock/us-gov-east-1/claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "bedrock/us-gov-west-1/amazon.nova-pro-v1:0",
    "provider": "bedrock",
    "input": 9.6E-7,
    "output": 0.00000384,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "bedrock/us-gov-west-1/anthropic.claude-3-7-sonnet-20250219-v1:0",
    "provider": "bedrock",
    "input": 0.0000036,
    "output": 0.000018,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-gov-west-1/anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.0000036,
    "output": 0.000018,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-gov-west-1/anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 3E-7,
    "output": 0.0000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "bedrock/us-gov-west-1/claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "bedrock/us-west-2/anthropic.claude-instant-v1",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.0000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-west-2/anthropic.claude-v1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-west-2/anthropic.claude-v2:1",
    "provider": "bedrock",
    "input": 0.000008,
    "output": 0.000024,
    "max_input_tokens": 100000,
    "max_output_tokens": 8191
  },
  {
    "model": "bedrock/us-west-2/deepseek.v3.2",
    "provider": "bedrock",
    "input": 6.2E-7,
    "output": 0.00000185,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "bedrock/us-west-2/minimax.minimax-m2.1",
    "provider": "bedrock",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us-west-2/moonshotai.kimi-k2-thinking",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-west-2/moonshotai.kimi-k2.5",
    "provider": "bedrock",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "bedrock/us-west-2/qwen.qwen3-coder-next",
    "provider": "bedrock",
    "input": 5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.000004,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "cerebras/llama-3.3-70b",
    "provider": "cerebras",
    "input": 8.5E-7,
    "output": 0.0000012,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "cerebras/llama3.1-70b",
    "provider": "cerebras",
    "input": 6E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "cerebras/llama3.1-8b",
    "provider": "cerebras",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "cerebras/gpt-oss-120b",
    "provider": "cerebras",
    "input": 3.5E-7,
    "output": 7.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "cerebras/qwen-3-32b",
    "provider": "cerebras",
    "input": 4E-7,
    "output": 8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "cerebras/zai-glm-4.6",
    "provider": "cerebras",
    "input": 0.00000225,
    "output": 0.00000275,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "cerebras/zai-glm-4.7",
    "provider": "cerebras",
    "input": 0.00000225,
    "output": 0.00000275,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "chatgpt-4o-latest",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "claude-3-5-haiku-20241022",
    "provider": "anthropic",
    "input": 8E-7,
    "output": 0.000004,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "claude-3-5-haiku-latest",
    "provider": "anthropic",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "claude-haiku-4-5-20251001",
    "provider": "anthropic",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-haiku-4-5",
    "provider": "anthropic",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-3-5-sonnet-20240620",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "claude-3-5-sonnet-20241022",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "claude-3-5-sonnet-latest",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "claude-3-7-sonnet-20250219",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-3-7-sonnet-latest",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-3-haiku-20240307",
    "provider": "anthropic",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "claude-3-opus-20240229",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "claude-3-opus-latest",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "claude-4-opus-20250514",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "claude-4-sonnet-20250514",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-sonnet-4-5",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-sonnet-4-5-20250929",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-sonnet-4-6",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "us/claude-sonnet-4-6",
    "provider": "anthropic",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-opus-4-1",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "claude-opus-4-1-20250805",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "claude-opus-4-20250514",
    "provider": "anthropic",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "claude-opus-4-5-20251101",
    "provider": "anthropic",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-opus-4-5",
    "provider": "anthropic",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "claude-opus-4-6",
    "provider": "anthropic",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "fast/claude-opus-4-6",
    "provider": "anthropic",
    "input": 0.00003,
    "output": 0.00015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "us/claude-opus-4-6",
    "provider": "anthropic",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "fast/us/claude-opus-4-6",
    "provider": "anthropic",
    "input": 0.00003,
    "output": 0.00015,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "claude-opus-4-6-20260205",
    "provider": "anthropic",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "fast/claude-opus-4-6-20260205",
    "provider": "anthropic",
    "input": 0.00003,
    "output": 0.00015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "us/claude-opus-4-6-20260205",
    "provider": "anthropic",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "claude-sonnet-4-20250514",
    "provider": "anthropic",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "codex-mini-latest",
    "provider": "openai",
    "input": 0.0000015,
    "output": 0.000006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "cohere.command-r-plus-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "cohere.command-r-v1:0",
    "provider": "bedrock",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "cohere.embed-v4:0",
    "provider": "bedrock",
    "input": 1.2E-7,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "cohere/embed-v4.0",
    "provider": "cohere",
    "input": 1.2E-7,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "command-a-03-2025",
    "provider": "cohere_chat",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 256000,
    "max_output_tokens": 8000
  },
  {
    "model": "command-r",
    "provider": "cohere_chat",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "command-r-08-2024",
    "provider": "cohere_chat",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "command-r-plus",
    "provider": "cohere_chat",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "command-r-plus-08-2024",
    "provider": "cohere_chat",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "command-r7b-12-2024",
    "provider": "cohere_chat",
    "input": 1.5E-7,
    "output": 3.75E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "deepseek-chat",
    "provider": "deepseek",
    "input": 2.8E-7,
    "output": 4.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "deepseek-reasoner",
    "provider": "deepseek",
    "input": 2.8E-7,
    "output": 4.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 65536
  },
  {
    "model": "dashscope/qwen-coder",
    "provider": "dashscope",
    "input": 3E-7,
    "output": 0.0000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-plus",
    "provider": "dashscope",
    "input": 4E-7,
    "output": 0.0000012,
    "max_input_tokens": 129024,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-plus-2025-01-25",
    "provider": "dashscope",
    "input": 4E-7,
    "output": 0.0000012,
    "max_input_tokens": 129024,
    "max_output_tokens": 8192
  },
  {
    "model": "dashscope/qwen-plus-2025-04-28",
    "provider": "dashscope",
    "input": 4E-7,
    "output": 0.0000012,
    "max_input_tokens": 129024,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-plus-2025-07-14",
    "provider": "dashscope",
    "input": 4E-7,
    "output": 0.0000012,
    "max_input_tokens": 129024,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-turbo",
    "provider": "dashscope",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 129024,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-turbo-2024-11-01",
    "provider": "dashscope",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "dashscope/qwen-turbo-2025-04-28",
    "provider": "dashscope",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 16384
  },
  {
    "model": "dashscope/qwen-turbo-latest",
    "provider": "dashscope",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 16384
  },
  {
    "model": "databricks/databricks-claude-3-7-sonnet",
    "provider": "databricks",
    "input": 0.0000029999900000000002,
    "output": 0.000015000020000000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-claude-haiku-4-5",
    "provider": "databricks",
    "input": 0.00000100002,
    "output": 0.00000500003,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "databricks/databricks-claude-opus-4",
    "provider": "databricks",
    "input": 0.000015000020000000002,
    "output": 0.00007500003000000001,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "databricks/databricks-claude-opus-4-1",
    "provider": "databricks",
    "input": 0.000015000020000000002,
    "output": 0.00007500003000000001,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "databricks/databricks-claude-opus-4-5",
    "provider": "databricks",
    "input": 0.00000500003,
    "output": 0.000025000010000000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "databricks/databricks-claude-sonnet-4",
    "provider": "databricks",
    "input": 0.0000029999900000000002,
    "output": 0.000015000020000000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "databricks/databricks-claude-sonnet-4-1",
    "provider": "databricks",
    "input": 0.0000029999900000000002,
    "output": 0.000015000020000000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "databricks/databricks-claude-sonnet-4-5",
    "provider": "databricks",
    "input": 0.0000029999900000000002,
    "output": 0.000015000020000000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "databricks/databricks-gemini-2-5-flash",
    "provider": "databricks",
    "input": 3.0001999999999996E-7,
    "output": 0.00000249998,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "databricks/databricks-gemini-2-5-pro",
    "provider": "databricks",
    "input": 0.00000124999,
    "output": 0.000009999990000000002,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "databricks/databricks-gemma-3-12b",
    "provider": "databricks",
    "input": 1.5000999999999998E-7,
    "output": 5.0001E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "databricks/databricks-gpt-5",
    "provider": "databricks",
    "input": 0.00000124999,
    "output": 0.000009999990000000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-gpt-5-1",
    "provider": "databricks",
    "input": 0.00000124999,
    "output": 0.000009999990000000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-gpt-5-mini",
    "provider": "databricks",
    "input": 2.4997000000000006E-7,
    "output": 0.0000019999700000000004,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-gpt-5-nano",
    "provider": "databricks",
    "input": 4.998E-8,
    "output": 3.9998000000000007E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-gpt-oss-120b",
    "provider": "databricks",
    "input": 1.5000999999999998E-7,
    "output": 5.9997E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "databricks/databricks-gpt-oss-20b",
    "provider": "databricks",
    "input": 7E-8,
    "output": 3.0001999999999996E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "databricks/databricks-llama-4-maverick",
    "provider": "databricks",
    "input": 5.0001E-7,
    "output": 0.0000015000300000000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-meta-llama-3-1-405b-instruct",
    "provider": "databricks",
    "input": 0.00000500003,
    "output": 0.000015000020000000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-meta-llama-3-1-8b-instruct",
    "provider": "databricks",
    "input": 1.5000999999999998E-7,
    "output": 4.5003000000000007E-7,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-meta-llama-3-3-70b-instruct",
    "provider": "databricks",
    "input": 5.0001E-7,
    "output": 0.0000015000300000000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "databricks/databricks-meta-llama-3-70b-instruct",
    "provider": "databricks",
    "input": 0.00000100002,
    "output": 0.0000029999900000000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "deepinfra/NousResearch/Hermes-3-Llama-3.1-405B",
    "provider": "deepinfra",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/NousResearch/Hermes-3-Llama-3.1-70B",
    "provider": "deepinfra",
    "input": 3E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/Qwen/QwQ-32B",
    "provider": "deepinfra",
    "input": 1.5E-7,
    "output": 4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/Qwen/Qwen2.5-VL-32B-Instruct",
    "provider": "deepinfra",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "deepinfra/Qwen/Qwen3-235B-A22B-Instruct-2507",
    "provider": "deepinfra",
    "input": 9E-8,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Qwen/Qwen3-235B-A22B-Thinking-2507",
    "provider": "deepinfra",
    "input": 3E-7,
    "output": 0.0000029,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "provider": "deepinfra",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Qwen/Qwen3-Coder-480B-A35B-Instruct-Turbo",
    "provider": "deepinfra",
    "input": 2.9E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Qwen/Qwen3-Next-80B-A3B-Instruct",
    "provider": "deepinfra",
    "input": 1.4E-7,
    "output": 0.0000014,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Qwen/Qwen3-Next-80B-A3B-Thinking",
    "provider": "deepinfra",
    "input": 1.4E-7,
    "output": 0.0000014,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/Sao10K/L3.1-70B-Euryale-v2.2",
    "provider": "deepinfra",
    "input": 6.5E-7,
    "output": 7.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/Sao10K/L3.3-70B-Euryale-v2.3",
    "provider": "deepinfra",
    "input": 6.5E-7,
    "output": 7.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/anthropic/claude-3-7-sonnet-latest",
    "provider": "deepinfra",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "deepinfra/anthropic/claude-4-opus",
    "provider": "deepinfra",
    "input": 0.0000165,
    "output": 0.0000825,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "deepinfra/anthropic/claude-4-sonnet",
    "provider": "deepinfra",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-R1",
    "provider": "deepinfra",
    "input": 7E-7,
    "output": 0.0000024,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-R1-0528",
    "provider": "deepinfra",
    "input": 5E-7,
    "output": 0.00000215,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "provider": "deepinfra",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "provider": "deepinfra",
    "input": 2.7E-7,
    "output": 2.7E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-V3",
    "provider": "deepinfra",
    "input": 3.8E-7,
    "output": 8.9E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-V3-0324",
    "provider": "deepinfra",
    "input": 2.5E-7,
    "output": 8.8E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-V3.1",
    "provider": "deepinfra",
    "input": 2.7E-7,
    "output": 0.000001,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/deepseek-ai/DeepSeek-V3.1-Terminus",
    "provider": "deepinfra",
    "input": 2.7E-7,
    "output": 0.000001,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/google/gemini-2.0-flash-001",
    "provider": "deepinfra",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "deepinfra/google/gemini-2.5-flash",
    "provider": "deepinfra",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "deepinfra/google/gemini-2.5-pro",
    "provider": "deepinfra",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "deepinfra/google/gemma-3-12b-it",
    "provider": "deepinfra",
    "input": 5E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/google/gemma-3-27b-it",
    "provider": "deepinfra",
    "input": 9E-8,
    "output": 1.6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/google/gemma-3-4b-it",
    "provider": "deepinfra",
    "input": 4E-8,
    "output": 8E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-3.2-11B-Vision-Instruct",
    "provider": "deepinfra",
    "input": 4.9E-8,
    "output": 4.9E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-3.2-3B-Instruct",
    "provider": "deepinfra",
    "input": 2E-8,
    "output": 2E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-3.3-70B-Instruct",
    "provider": "deepinfra",
    "input": 2.3E-7,
    "output": 4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-3.3-70B-Instruct-Turbo",
    "provider": "deepinfra",
    "input": 1.3E-7,
    "output": 3.9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "provider": "deepinfra",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 1048576
  },
  {
    "model": "deepinfra/meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "provider": "deepinfra",
    "input": 8E-8,
    "output": 3E-7,
    "max_input_tokens": 327680,
    "max_output_tokens": 327680
  },
  {
    "model": "deepinfra/meta-llama/Llama-Guard-3-8B",
    "provider": "deepinfra",
    "input": 5.5E-8,
    "output": 5.5E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Llama-Guard-4-12B",
    "provider": "deepinfra",
    "input": 1.8E-7,
    "output": 1.8E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct",
    "provider": "deepinfra",
    "input": 4E-7,
    "output": 4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
    "provider": "deepinfra",
    "input": 1E-7,
    "output": 2.8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct",
    "provider": "deepinfra",
    "input": 3E-8,
    "output": 5E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
    "provider": "deepinfra",
    "input": 2E-8,
    "output": 3E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/mistralai/Mistral-Nemo-Instruct-2407",
    "provider": "deepinfra",
    "input": 2E-8,
    "output": 4E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "provider": "deepinfra",
    "input": 7.5E-8,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "deepinfra/moonshotai/Kimi-K2-Instruct",
    "provider": "deepinfra",
    "input": 5E-7,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/moonshotai/Kimi-K2-Instruct-0905",
    "provider": "deepinfra",
    "input": 5E-7,
    "output": 0.000002,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "deepinfra/nvidia/Llama-3.1-Nemotron-70B-Instruct",
    "provider": "deepinfra",
    "input": 6E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/nvidia/Llama-3.3-Nemotron-Super-49B-v1.5",
    "provider": "deepinfra",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/nvidia/NVIDIA-Nemotron-Nano-9B-v2",
    "provider": "deepinfra",
    "input": 4E-8,
    "output": 1.6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/openai/gpt-oss-120b",
    "provider": "deepinfra",
    "input": 5E-8,
    "output": 4.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/openai/gpt-oss-20b",
    "provider": "deepinfra",
    "input": 4E-8,
    "output": 1.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepinfra/zai-org/GLM-4.5",
    "provider": "deepinfra",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "deepseek/deepseek-chat",
    "provider": "deepseek",
    "input": 2.8E-7,
    "output": 4.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "deepseek/deepseek-coder",
    "provider": "deepseek",
    "input": 1.4E-7,
    "output": 2.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "deepseek/deepseek-reasoner",
    "provider": "deepseek",
    "input": 2.8E-7,
    "output": 4.2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 65536
  },
  {
    "model": "deepseek/deepseek-v3.2",
    "provider": "deepseek",
    "input": 2.8E-7,
    "output": 4E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "deepseek.v3-v1:0",
    "provider": "bedrock_converse",
    "input": 5.8E-7,
    "output": 0.00000168,
    "max_input_tokens": 163840,
    "max_output_tokens": 81920
  },
  {
    "model": "deepseek.v3.2",
    "provider": "bedrock_converse",
    "input": 6.2E-7,
    "output": 0.00000185,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "glm-4-7-251222",
    "provider": "volcengine",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 204800,
    "max_output_tokens": 131072
  },
  {
    "model": "kimi-k2-thinking-251104",
    "provider": "volcengine",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 229376,
    "max_output_tokens": 32768
  },
  {
    "model": "eu.amazon.nova-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 7.8E-8,
    "output": 3.12E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "eu.amazon.nova-micro-v1:0",
    "provider": "bedrock_converse",
    "input": 4.6E-8,
    "output": 1.84E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 10000
  },
  {
    "model": "eu.amazon.nova-pro-v1:0",
    "provider": "bedrock_converse",
    "input": 0.00000105,
    "output": 0.0000042,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "eu.anthropic.claude-3-5-haiku-20241022-v1:0",
    "provider": "bedrock",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "eu.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000011,
    "output": 0.0000055,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.anthropic.claude-3-5-sonnet-20241022-v2:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "eu.anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.anthropic.claude-3-opus-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.anthropic.claude-3-sonnet-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.anthropic.claude-opus-4-1-20250805-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "eu.anthropic.claude-opus-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "eu.anthropic.claude-sonnet-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.meta.llama3-2-1b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1.3E-7,
    "output": 1.3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.meta.llama3-2-3b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1.9E-7,
    "output": 1.9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "eu.mistral.pixtral-large-2502-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1",
    "provider": "fireworks_ai",
    "input": 0.000003,
    "output": 0.000008,
    "max_input_tokens": 128000,
    "max_output_tokens": 20480
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-0528",
    "provider": "fireworks_ai",
    "input": 0.000003,
    "output": 0.000008,
    "max_input_tokens": 160000,
    "max_output_tokens": 160000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-basic",
    "provider": "fireworks_ai",
    "input": 5.5E-7,
    "output": 0.00000219,
    "max_input_tokens": 128000,
    "max_output_tokens": 20480
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3-0324",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3p1",
    "provider": "fireworks_ai",
    "input": 5.6E-7,
    "output": 0.00000168,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3p1-terminus",
    "provider": "fireworks_ai",
    "input": 5.6E-7,
    "output": 0.00000168,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3p2",
    "provider": "fireworks_ai",
    "input": 5.6E-7,
    "output": 0.00000168,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/glm-4p5",
    "provider": "fireworks_ai",
    "input": 5.5E-7,
    "output": 0.00000219,
    "max_input_tokens": 128000,
    "max_output_tokens": 96000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/glm-4p5-air",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 96000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/glm-4p6",
    "provider": "fireworks_ai",
    "input": 5.5E-7,
    "output": 0.00000219,
    "max_input_tokens": 202800,
    "max_output_tokens": 202800
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/glm-4p7",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 202800,
    "max_output_tokens": 202800
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-120b",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-20b",
    "provider": "fireworks_ai",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kimi-k2-instruct-0905",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 32768
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kimi-k2-thinking",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kimi-k2p5",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct",
    "provider": "fireworks_ai",
    "input": 0.000003,
    "output": 0.000003,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/minimax-m2p1",
    "provider": "fireworks_ai",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 204800,
    "max_output_tokens": 204800
  },
  {
    "model": "fireworks_ai/glm-4p7",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 202800,
    "max_output_tokens": 202800
  },
  {
    "model": "fireworks_ai/kimi-k2p5",
    "provider": "fireworks_ai",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/minimax-m2p1",
    "provider": "fireworks_ai",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 204800,
    "max_output_tokens": 204800
  },
  {
    "model": "ft:gpt-4o-2024-08-06",
    "provider": "openai",
    "input": 0.00000375,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "ft:gpt-4o-2024-11-20",
    "provider": "openai",
    "input": 0.00000375,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "ft:gpt-4o-mini-2024-07-18",
    "provider": "openai",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "ft:gpt-4.1-2025-04-14",
    "provider": "openai",
    "input": 0.000003,
    "output": 0.000012,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "ft:gpt-4.1-mini-2025-04-14",
    "provider": "openai",
    "input": 8E-7,
    "output": 0.0000032,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "ft:gpt-4.1-nano-2025-04-14",
    "provider": "openai",
    "input": 2E-7,
    "output": 8E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "ft:o4-mini-2025-04-16",
    "provider": "openai",
    "input": 0.000004,
    "output": 0.000016,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "gemini-1.5-flash",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-flash-001",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-flash-002",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-flash-exp-0827",
    "provider": "vertex_ai-language-models",
    "input": 4.688E-9,
    "output": 4.6875E-9,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-flash-preview-0514",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 4.6875E-9,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.000005,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro-001",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.000005,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro-002",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.000005,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro-preview-0215",
    "provider": "vertex_ai-language-models",
    "input": 7.8125E-8,
    "output": 3.125E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro-preview-0409",
    "provider": "vertex_ai-language-models",
    "input": 7.8125E-8,
    "output": 3.125E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-1.5-pro-preview-0514",
    "provider": "vertex_ai-language-models",
    "input": 7.8125E-8,
    "output": 3.125E-7,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash",
    "provider": "vertex_ai-language-models",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-001",
    "provider": "vertex_ai-language-models",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-exp",
    "provider": "vertex_ai-language-models",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-lite",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-lite-001",
    "provider": "vertex_ai-language-models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-live-preview-04-09",
    "provider": "vertex_ai-language-models",
    "input": 5E-7,
    "output": 0.000002,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.0-flash-preview-image-generation",
    "provider": "vertex_ai-language-models",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-thinking-exp",
    "provider": "vertex_ai-language-models",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.0-flash-thinking-exp-01-21",
    "provider": "vertex_ai-language-models",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini-2.0-pro-exp-02-05",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.5-flash",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-image-preview",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.00003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-lite",
    "provider": "vertex_ai-language-models",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-lite-preview-09-2025",
    "provider": "vertex_ai-language-models",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-preview-09-2025",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-live-2.5-flash-preview-native-audio-09-2025",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.000002,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-live-2.5-flash-preview-native-audio-09-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.000002,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-lite-preview-06-17",
    "provider": "vertex_ai-language-models",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-preview-04-17",
    "provider": "vertex_ai-language-models",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-flash-preview-05-20",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-pro",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-3-pro-preview",
    "provider": "vertex_ai-language-models",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-3.1-pro-preview",
    "provider": "vertex_ai-language-models",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini-3.1-pro-preview-customtools",
    "provider": "vertex_ai-language-models",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "vertex_ai/gemini-3-pro-preview",
    "provider": "vertex_ai",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "vertex_ai/gemini-3-flash-preview",
    "provider": "vertex_ai",
    "input": 5E-7,
    "output": 0.000003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "vertex_ai/gemini-3.1-pro-preview",
    "provider": "vertex_ai",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "vertex_ai/gemini-3.1-pro-preview-customtools",
    "provider": "vertex_ai",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini-2.5-pro-exp-03-25",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-pro-preview-03-25",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-pro-preview-05-06",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-pro-preview-06-05",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-pro-preview-tts",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-robotics-er-1.5-preview",
    "provider": "vertex_ai-language-models",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-robotics-er-1.5-preview",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-2.5-computer-use-preview-10-2025",
    "provider": "vertex_ai-language-models",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 64000
  },
  {
    "model": "gemini-flash-experimental",
    "provider": "vertex_ai-language-models",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-pro-experimental",
    "provider": "vertex_ai-language-models",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-001",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-002",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-8b",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-8b-exp-0827",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-8b-exp-0924",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-exp-0827",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-flash-latest",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro",
    "provider": "gemini",
    "input": 0.0000035,
    "output": 0.0000105,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro-001",
    "provider": "gemini",
    "input": 0.0000035,
    "output": 0.0000105,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro-002",
    "provider": "gemini",
    "input": 0.0000035,
    "output": 0.0000105,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro-exp-0801",
    "provider": "gemini",
    "input": 0.0000035,
    "output": 0.0000105,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro-exp-0827",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-1.5-pro-latest",
    "provider": "gemini",
    "input": 0.0000035,
    "output": 0.00000105,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-001",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-exp",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-lite",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-lite-preview-02-05",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-live-001",
    "provider": "gemini",
    "input": 3.5E-7,
    "output": 0.0000015,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.0-flash-preview-image-generation",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.0-flash-thinking-exp",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini/gemini-2.0-flash-thinking-exp-01-21",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini/gemini-2.0-pro-exp-02-05",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.5-flash",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-image-preview",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.00003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-lite",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-lite-preview-09-2025",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-preview-09-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-flash-latest",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-flash-lite-latest",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-lite-preview-06-17",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-preview-04-17",
    "provider": "gemini",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-flash-preview-05-20",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-computer-use-preview-10-2025",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 64000
  },
  {
    "model": "gemini/gemini-3-pro-preview",
    "provider": "gemini",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-3-flash-preview",
    "provider": "gemini",
    "input": 5E-7,
    "output": 0.000003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-3.1-pro-preview",
    "provider": "gemini",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini/gemini-3.1-pro-preview-customtools",
    "provider": "gemini",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gemini-3-flash-preview",
    "provider": "vertex_ai-language-models",
    "input": 5E-7,
    "output": 0.000003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro-exp-03-25",
    "provider": "gemini",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro-preview-03-25",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro-preview-05-06",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro-preview-06-05",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-2.5-pro-preview-tts",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-exp-1114",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-exp-1206",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 2097152,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemma-3-27b-it",
    "provider": "gemini",
    "input": 0,
    "output": 0,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "gigachat/GigaChat-2-Lite",
    "provider": "gigachat",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "gigachat/GigaChat-2-Max",
    "provider": "gigachat",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "gigachat/GigaChat-2-Pro",
    "provider": "gigachat",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "gmi/anthropic/claude-opus-4.5",
    "provider": "gmi",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/anthropic/claude-sonnet-4.5",
    "provider": "gmi",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/anthropic/claude-sonnet-4",
    "provider": "gmi",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/anthropic/claude-opus-4",
    "provider": "gmi",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/openai/gpt-5.2",
    "provider": "gmi",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/openai/gpt-5.1",
    "provider": "gmi",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/openai/gpt-5",
    "provider": "gmi",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 409600,
    "max_output_tokens": 32000
  },
  {
    "model": "gmi/openai/gpt-4o",
    "provider": "gmi",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/openai/gpt-4o-mini",
    "provider": "gmi",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/deepseek-ai/DeepSeek-V3.2",
    "provider": "gmi",
    "input": 2.8E-7,
    "output": 4E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/deepseek-ai/DeepSeek-V3-0324",
    "provider": "gmi",
    "input": 2.8E-7,
    "output": 8.8E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/google/gemini-3-pro-preview",
    "provider": "gmi",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gmi/google/gemini-3-flash-preview",
    "provider": "gmi",
    "input": 5E-7,
    "output": 0.000003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "gmi/moonshotai/Kimi-K2-Thinking",
    "provider": "gmi",
    "input": 8E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/MiniMaxAI/MiniMax-M2.1",
    "provider": "gmi",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196608,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/Qwen/Qwen3-VL-235B-A22B-Instruct-FP8",
    "provider": "gmi",
    "input": 3E-7,
    "output": 0.0000014,
    "max_input_tokens": 262144,
    "max_output_tokens": 16384
  },
  {
    "model": "gmi/zai-org/GLM-4.7-FP8",
    "provider": "gmi",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 202752,
    "max_output_tokens": 16384
  },
  {
    "model": "google.gemma-3-12b-it",
    "provider": "bedrock_converse",
    "input": 9E-8,
    "output": 2.9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "google.gemma-3-27b-it",
    "provider": "bedrock_converse",
    "input": 2.3E-7,
    "output": 3.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "google.gemma-3-4b-it",
    "provider": "bedrock_converse",
    "input": 4E-8,
    "output": 8E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "global.anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "global.anthropic.claude-sonnet-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "global.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "global.amazon.nova-2-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "gpt-4-0125-preview",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-1106-preview",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-1106-vision-preview",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-turbo",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-turbo-2024-04-09",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-turbo-preview",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4-vision-preview",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4.1",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.1-2025-04-14",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.1-mini",
    "provider": "openai",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.1-mini-2025-04-14",
    "provider": "openai",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.1-nano",
    "provider": "openai",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.1-nano-2025-04-14",
    "provider": "openai",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "gpt-4.5-preview",
    "provider": "openai",
    "input": 0.000075,
    "output": 0.00015,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4.5-preview-2025-02-27",
    "provider": "openai",
    "input": 0.000075,
    "output": 0.00015,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-2024-05-13",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-2024-08-06",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-2024-11-20",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-audio-preview",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-audio-preview-2024-10-01",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-audio-preview-2024-12-17",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-audio-preview-2025-06-03",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-audio",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-audio-2025-08-28",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-audio-mini",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-audio-mini-2025-10-06",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-audio-mini-2025-12-15",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini-2024-07-18",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini-audio-preview",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini-audio-preview-2024-12-17",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini-realtime-preview",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-mini-realtime-preview-2024-12-17",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-mini-search-preview",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-mini-search-preview-2025-03-11",
    "provider": "openai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-realtime-preview",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-realtime-preview-2024-10-01",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-realtime-preview-2024-12-17",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-realtime-preview-2025-06-03",
    "provider": "openai",
    "input": 0.000005,
    "output": 0.00002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-4o-search-preview",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-4o-search-preview-2025-03-11",
    "provider": "openai",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-5",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1-2025-11-13",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1-chat-latest",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-5.2",
    "provider": "openai",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.2-2025-12-11",
    "provider": "openai",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.2-chat-latest",
    "provider": "openai",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-5.2-pro",
    "provider": "openai",
    "input": 0.000021,
    "output": 0.000168,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.2-pro-2025-12-11",
    "provider": "openai",
    "input": 0.000021,
    "output": 0.000168,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-pro",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00012,
    "max_input_tokens": 128000,
    "max_output_tokens": 272000
  },
  {
    "model": "gpt-5-pro-2025-10-06",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00012,
    "max_input_tokens": 128000,
    "max_output_tokens": 272000
  },
  {
    "model": "gpt-5-2025-08-07",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-chat",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-5-chat-latest",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "gpt-5-codex",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1-codex",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1-codex-max",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.1-codex-mini",
    "provider": "openai",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5.2-codex",
    "provider": "openai",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-mini",
    "provider": "openai",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-mini-2025-08-07",
    "provider": "openai",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-nano",
    "provider": "openai",
    "input": 5E-8,
    "output": 4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-nano-2025-08-07",
    "provider": "openai",
    "input": 5E-8,
    "output": 4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-realtime-mini",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "lemonade/Qwen3-Coder-30B-A3B-Instruct-GGUF",
    "provider": "lemonade",
    "input": 0,
    "output": 0,
    "max_input_tokens": 262144,
    "max_output_tokens": 32768
  },
  {
    "model": "lemonade/gpt-oss-20b-mxfp4-GGUF",
    "provider": "lemonade",
    "input": 0,
    "output": 0,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "lemonade/gpt-oss-120b-mxfp-GGUF",
    "provider": "lemonade",
    "input": 0,
    "output": 0,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "lemonade/Gemma-3-4b-it-GGUF",
    "provider": "lemonade",
    "input": 0,
    "output": 0,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "lemonade/Qwen3-4B-Instruct-2507-GGUF",
    "provider": "lemonade",
    "input": 0,
    "output": 0,
    "max_input_tokens": 262144,
    "max_output_tokens": 32768
  },
  {
    "model": "amazon-nova/nova-micro-v1",
    "provider": "amazon_nova",
    "input": 3.5E-8,
    "output": 1.4E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 10000
  },
  {
    "model": "amazon-nova/nova-lite-v1",
    "provider": "amazon_nova",
    "input": 6E-8,
    "output": 2.4E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "amazon-nova/nova-premier-v1",
    "provider": "amazon_nova",
    "input": 0.0000025,
    "output": 0.0000125,
    "max_input_tokens": 1000000,
    "max_output_tokens": 10000
  },
  {
    "model": "amazon-nova/nova-pro-v1",
    "provider": "amazon_nova",
    "input": 8E-7,
    "output": 0.0000032,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "groq/llama-3.1-8b-instant",
    "provider": "groq",
    "input": 5E-8,
    "output": 8E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "groq/llama-3.3-70b-versatile",
    "provider": "groq",
    "input": 5.9E-7,
    "output": 7.9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "groq/meta-llama/llama-4-maverick-17b-128e-instruct",
    "provider": "groq",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "groq/meta-llama/llama-4-scout-17b-16e-instruct",
    "provider": "groq",
    "input": 1.1E-7,
    "output": 3.4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "groq/moonshotai/kimi-k2-instruct-0905",
    "provider": "groq",
    "input": 0.000001,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 16384
  },
  {
    "model": "groq/openai/gpt-oss-120b",
    "provider": "groq",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32766
  },
  {
    "model": "groq/openai/gpt-oss-20b",
    "provider": "groq",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "groq/qwen/qwen3-32b",
    "provider": "groq",
    "input": 2.9E-7,
    "output": 5.9E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "hyperbolic/Qwen/QwQ-32B",
    "provider": "hyperbolic",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/Qwen/Qwen2.5-72B-Instruct",
    "provider": "hyperbolic",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/Qwen/Qwen3-235B-A22B",
    "provider": "hyperbolic",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/deepseek-ai/DeepSeek-R1-0528",
    "provider": "hyperbolic",
    "input": 2.5E-7,
    "output": 2.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/meta-llama/Llama-3.3-70B-Instruct",
    "provider": "hyperbolic",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/meta-llama/Meta-Llama-3-70B-Instruct",
    "provider": "hyperbolic",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "hyperbolic/moonshotai/Kimi-K2-Instruct",
    "provider": "hyperbolic",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "jamba-1.5",
    "provider": "ai21",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-1.5-large",
    "provider": "ai21",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-1.5-large@001",
    "provider": "ai21",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-1.5-mini",
    "provider": "ai21",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-1.5-mini@001",
    "provider": "ai21",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-large-1.6",
    "provider": "ai21",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-large-1.7",
    "provider": "ai21",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-mini-1.6",
    "provider": "ai21",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jamba-mini-1.7",
    "provider": "ai21",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "jp.anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "jp.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000011,
    "output": 0.0000055,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "lambda_ai/deepseek-llama3.3-70b",
    "provider": "lambda_ai",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/deepseek-r1-0528",
    "provider": "lambda_ai",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/deepseek-r1-671b",
    "provider": "lambda_ai",
    "input": 8E-7,
    "output": 8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/deepseek-v3-0324",
    "provider": "lambda_ai",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/hermes3-405b",
    "provider": "lambda_ai",
    "input": 8E-7,
    "output": 8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/hermes3-70b",
    "provider": "lambda_ai",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/hermes3-8b",
    "provider": "lambda_ai",
    "input": 2.5E-8,
    "output": 4E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/lfm-40b",
    "provider": "lambda_ai",
    "input": 1E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/lfm-7b",
    "provider": "lambda_ai",
    "input": 2.5E-8,
    "output": 4E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama-4-maverick-17b-128e-instruct-fp8",
    "provider": "lambda_ai",
    "input": 5E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "lambda_ai/llama3.1-405b-instruct-fp8",
    "provider": "lambda_ai",
    "input": 8E-7,
    "output": 8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.1-70b-instruct-fp8",
    "provider": "lambda_ai",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.1-8b-instruct",
    "provider": "lambda_ai",
    "input": 2.5E-8,
    "output": 4E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.1-nemotron-70b-instruct-fp8",
    "provider": "lambda_ai",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.2-11b-vision-instruct",
    "provider": "lambda_ai",
    "input": 1.5E-8,
    "output": 2.5E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.2-3b-instruct",
    "provider": "lambda_ai",
    "input": 1.5E-8,
    "output": 2.5E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/llama3.3-70b-instruct-fp8",
    "provider": "lambda_ai",
    "input": 1.2E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/qwen25-coder-32b-instruct",
    "provider": "lambda_ai",
    "input": 5E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "lambda_ai/qwen3-32b-fp8",
    "provider": "lambda_ai",
    "input": 5E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "meta.llama3-1-405b-instruct-v1:0",
    "provider": "bedrock",
    "input": 0.00000532,
    "output": 0.000016,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama3-1-70b-instruct-v1:0",
    "provider": "bedrock",
    "input": 9.9E-7,
    "output": 9.9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "meta.llama3-1-8b-instruct-v1:0",
    "provider": "bedrock",
    "input": 2.2E-7,
    "output": 2.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "meta.llama3-2-11b-instruct-v1:0",
    "provider": "bedrock",
    "input": 3.5E-7,
    "output": 3.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama3-2-1b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama3-2-3b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama3-2-90b-instruct-v1:0",
    "provider": "bedrock",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama3-3-70b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama4-maverick-17b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 2.4E-7,
    "output": 9.7E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "meta.llama4-scout-17b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 1.7E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "minimax.minimax-m2",
    "provider": "bedrock_converse",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax.minimax-m2.1",
    "provider": "bedrock_converse",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax/MiniMax-M2.1",
    "provider": "minimax",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax/MiniMax-M2.1-lightning",
    "provider": "minimax",
    "input": 3E-7,
    "output": 0.0000024,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax/MiniMax-M2.5",
    "provider": "minimax",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax/MiniMax-M2.5-lightning",
    "provider": "minimax",
    "input": 3E-7,
    "output": 0.0000024,
    "max_input_tokens": 1000000,
    "max_output_tokens": 8192
  },
  {
    "model": "minimax/MiniMax-M2",
    "provider": "minimax",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.magistral-small-2509",
    "provider": "bedrock_converse",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.ministral-3-14b-instruct",
    "provider": "bedrock_converse",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.ministral-3-3b-instruct",
    "provider": "bedrock_converse",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.ministral-3-8b-instruct",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.mistral-large-2407-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000009,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "mistral.mistral-large-3-675b-instruct",
    "provider": "bedrock_converse",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.voxtral-mini-3b-2507",
    "provider": "bedrock_converse",
    "input": 4E-8,
    "output": 4E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral.voxtral-small-24b-2507",
    "provider": "bedrock_converse",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "mistral/codestral-2508",
    "provider": "mistral",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/codestral-mamba-latest",
    "provider": "mistral",
    "input": 2.5E-7,
    "output": 2.5E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/devstral-medium-2507",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/devstral-small-2505",
    "provider": "mistral",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/devstral-small-2507",
    "provider": "mistral",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/devstral-small-latest",
    "provider": "mistral",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/labs-devstral-small-2512",
    "provider": "mistral",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/devstral-latest",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/devstral-medium-latest",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/devstral-2512",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/mistral-large-2407",
    "provider": "mistral",
    "input": 0.000003,
    "output": 0.000009,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/mistral-large-2411",
    "provider": "mistral",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/mistral-large-latest",
    "provider": "mistral",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/mistral-large-3",
    "provider": "mistral",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 8191
  },
  {
    "model": "mistral/mistral-medium-2505",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 8191
  },
  {
    "model": "mistral/mistral-medium-latest",
    "provider": "mistral",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 131072,
    "max_output_tokens": 8191
  },
  {
    "model": "mistral/open-codestral-mamba",
    "provider": "mistral",
    "input": 2.5E-7,
    "output": 2.5E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "mistral/open-mistral-nemo",
    "provider": "mistral",
    "input": 3E-7,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/open-mistral-nemo-2407",
    "provider": "mistral",
    "input": 3E-7,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/pixtral-12b-2409",
    "provider": "mistral",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/pixtral-large-2411",
    "provider": "mistral",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "mistral/pixtral-large-latest",
    "provider": "mistral",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "moonshot.kimi-k2-thinking",
    "provider": "bedrock_converse",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "moonshotai.kimi-k2.5",
    "provider": "bedrock_converse",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/kimi-k2-0711-preview",
    "provider": "moonshot",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/kimi-k2-0905-preview",
    "provider": "moonshot",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/kimi-k2-turbo-preview",
    "provider": "moonshot",
    "input": 0.00000115,
    "output": 0.000008,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/kimi-k2.5",
    "provider": "moonshot",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/kimi-latest",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/kimi-latest-128k",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/kimi-thinking-preview",
    "provider": "moonshot",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/kimi-k2-thinking",
    "provider": "moonshot",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/kimi-k2-thinking-turbo",
    "provider": "moonshot",
    "input": 0.00000115,
    "output": 0.000008,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "moonshot/moonshot-v1-128k",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/moonshot-v1-128k-0430",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/moonshot-v1-128k-vision-preview",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "moonshot/moonshot-v1-auto",
    "provider": "moonshot",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "nvidia.nemotron-nano-12b-v2",
    "provider": "bedrock_converse",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "nvidia.nemotron-nano-9b-v2",
    "provider": "bedrock_converse",
    "input": 6E-8,
    "output": 2.3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "nvidia.nemotron-nano-3-30b",
    "provider": "bedrock_converse",
    "input": 6E-8,
    "output": 2.4E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "o1",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o1-2024-12-17",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o1-mini",
    "provider": "openai",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "o1-mini-2024-09-12",
    "provider": "openai",
    "input": 0.000003,
    "output": 0.000012,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "o1-preview",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "o1-preview-2024-09-12",
    "provider": "openai",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "o1-pro",
    "provider": "openai",
    "input": 0.00015,
    "output": 0.0006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o1-pro-2025-03-19",
    "provider": "openai",
    "input": 0.00015,
    "output": 0.0006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-2025-04-16",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-deep-research",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00004,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-deep-research-2025-06-26",
    "provider": "openai",
    "input": 0.00001,
    "output": 0.00004,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-mini",
    "provider": "openai",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-mini-2025-01-31",
    "provider": "openai",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-pro",
    "provider": "openai",
    "input": 0.00002,
    "output": 0.00008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o3-pro-2025-06-10",
    "provider": "openai",
    "input": 0.00002,
    "output": 0.00008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o4-mini",
    "provider": "openai",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o4-mini-2025-04-16",
    "provider": "openai",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o4-mini-deep-research",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "o4-mini-deep-research-2025-06-26",
    "provider": "openai",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "oci/meta.llama-3.1-405b-instruct",
    "provider": "oci",
    "input": 0.00001068,
    "output": 0.00001068,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/meta.llama-3.2-90b-vision-instruct",
    "provider": "oci",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/meta.llama-3.3-70b-instruct",
    "provider": "oci",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/meta.llama-4-maverick-17b-128e-instruct-fp8",
    "provider": "oci",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 512000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/meta.llama-4-scout-17b-16e-instruct",
    "provider": "oci",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 192000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/xai.grok-3",
    "provider": "oci",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "oci/xai.grok-3-fast",
    "provider": "oci",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "oci/xai.grok-3-mini",
    "provider": "oci",
    "input": 3E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "oci/xai.grok-3-mini-fast",
    "provider": "oci",
    "input": 6E-7,
    "output": 0.000004,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "oci/xai.grok-4",
    "provider": "oci",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "oci/cohere.command-latest",
    "provider": "oci",
    "input": 0.00000156,
    "output": 0.00000156,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/cohere.command-a-03-2025",
    "provider": "oci",
    "input": 0.00000156,
    "output": 0.00000156,
    "max_input_tokens": 256000,
    "max_output_tokens": 4000
  },
  {
    "model": "oci/cohere.command-plus-latest",
    "provider": "oci",
    "input": 0.00000156,
    "output": 0.00000156,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "ollama/deepseek-v3.1:671b-cloud",
    "provider": "ollama",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "ollama/gpt-oss:120b-cloud",
    "provider": "ollama",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "ollama/gpt-oss:20b-cloud",
    "provider": "ollama",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "ollama/qwen3-coder:480b-cloud",
    "provider": "ollama",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openai.gpt-oss-120b-1:0",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "openai.gpt-oss-20b-1:0",
    "provider": "bedrock_converse",
    "input": 7E-8,
    "output": 3E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "openai.gpt-oss-safeguard-120b",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "openai.gpt-oss-safeguard-20b",
    "provider": "bedrock_converse",
    "input": 7E-8,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "openrouter/anthropic/claude-3.5-sonnet",
    "provider": "openrouter",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "openrouter/anthropic/claude-3.7-sonnet",
    "provider": "openrouter",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/anthropic/claude-opus-4",
    "provider": "openrouter",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "openrouter/anthropic/claude-opus-4.1",
    "provider": "openrouter",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "openrouter/anthropic/claude-sonnet-4",
    "provider": "openrouter",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "openrouter/anthropic/claude-opus-4.5",
    "provider": "openrouter",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "openrouter/anthropic/claude-sonnet-4.5",
    "provider": "openrouter",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "openrouter/anthropic/claude-haiku-4.5",
    "provider": "openrouter",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "openrouter/bytedance/ui-tars-1.5-7b",
    "provider": "openrouter",
    "input": 1E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 2048
  },
  {
    "model": "openrouter/deepseek/deepseek-chat-v3.1",
    "provider": "openrouter",
    "input": 2E-7,
    "output": 8E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "openrouter/deepseek/deepseek-v3.2",
    "provider": "openrouter",
    "input": 2.8E-7,
    "output": 4E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "openrouter/deepseek/deepseek-v3.2-exp",
    "provider": "openrouter",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "openrouter/google/gemini-2.0-flash-001",
    "provider": "openrouter",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "openrouter/google/gemini-2.5-flash",
    "provider": "openrouter",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "openrouter/google/gemini-2.5-pro",
    "provider": "openrouter",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "openrouter/google/gemini-3-pro-preview",
    "provider": "openrouter",
    "input": 0.000002,
    "output": 0.000012,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "openrouter/google/gemini-3-flash-preview",
    "provider": "openrouter",
    "input": 5E-7,
    "output": 0.000003,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "openrouter/minimax/minimax-m2",
    "provider": "openrouter",
    "input": 2.55E-7,
    "output": 0.00000102,
    "max_input_tokens": 204800,
    "max_output_tokens": 204800
  },
  {
    "model": "openrouter/mistralai/devstral-2512",
    "provider": "openrouter",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 65536
  },
  {
    "model": "openrouter/mistralai/ministral-3b-2512",
    "provider": "openrouter",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "openrouter/mistralai/ministral-8b-2512",
    "provider": "openrouter",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/mistralai/ministral-14b-2512",
    "provider": "openrouter",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/mistralai/mistral-large-2512",
    "provider": "openrouter",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/moonshotai/kimi-k2.5",
    "provider": "openrouter",
    "input": 6E-7,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/openai/gpt-4.1",
    "provider": "openrouter",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "openrouter/openai/gpt-4.1-mini",
    "provider": "openrouter",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "openrouter/openai/gpt-4.1-nano",
    "provider": "openrouter",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "openrouter/openai/gpt-4o",
    "provider": "openrouter",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "openrouter/openai/gpt-4o-2024-05-13",
    "provider": "openrouter",
    "input": 0.000005,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "openrouter/openai/gpt-5-chat",
    "provider": "openrouter",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "openrouter/openai/gpt-5-codex",
    "provider": "openrouter",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5.2-codex",
    "provider": "openrouter",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5",
    "provider": "openrouter",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5-mini",
    "provider": "openrouter",
    "input": 2.5E-7,
    "output": 0.000002,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5-nano",
    "provider": "openrouter",
    "input": 5E-8,
    "output": 4E-7,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5.2",
    "provider": "openrouter",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-5.2-chat",
    "provider": "openrouter",
    "input": 0.00000175,
    "output": 0.000014,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "openrouter/openai/gpt-5.2-pro",
    "provider": "openrouter",
    "input": 0.000021,
    "output": 0.000168,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "openrouter/openai/gpt-oss-120b",
    "provider": "openrouter",
    "input": 1.8E-7,
    "output": 8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "openrouter/openai/gpt-oss-20b",
    "provider": "openrouter",
    "input": 2E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "openrouter/openai/o1",
    "provider": "openrouter",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "openrouter/openai/o3-mini",
    "provider": "openrouter",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "openrouter/openai/o3-mini-high",
    "provider": "openrouter",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 128000,
    "max_output_tokens": 65536
  },
  {
    "model": "openrouter/qwen/qwen3-coder",
    "provider": "openrouter",
    "input": 2.2E-7,
    "output": 9.5E-7,
    "max_input_tokens": 262100,
    "max_output_tokens": 262100
  },
  {
    "model": "openrouter/qwen/qwen3-235b-a22b-2507",
    "provider": "openrouter",
    "input": 7.1E-8,
    "output": 1E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/qwen/qwen3-235b-a22b-thinking-2507",
    "provider": "openrouter",
    "input": 1.1E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "openrouter/switchpoint/router",
    "provider": "openrouter",
    "input": 8.5E-7,
    "output": 0.0000034,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "openrouter/x-ai/grok-4",
    "provider": "openrouter",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "openrouter/z-ai/glm-4.6",
    "provider": "openrouter",
    "input": 4E-7,
    "output": 0.00000175,
    "max_input_tokens": 202800,
    "max_output_tokens": 131000
  },
  {
    "model": "openrouter/z-ai/glm-4.6:exacto",
    "provider": "openrouter",
    "input": 4.5E-7,
    "output": 0.0000019,
    "max_input_tokens": 202800,
    "max_output_tokens": 131000
  },
  {
    "model": "openrouter/xiaomi/mimo-v2-flash",
    "provider": "openrouter",
    "input": 9E-8,
    "output": 2.9E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 16384
  },
  {
    "model": "openrouter/z-ai/glm-4.7",
    "provider": "openrouter",
    "input": 4E-7,
    "output": 0.0000015,
    "max_input_tokens": 202752,
    "max_output_tokens": 64000
  },
  {
    "model": "openrouter/z-ai/glm-4.7-flash",
    "provider": "openrouter",
    "input": 7E-8,
    "output": 4E-7,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "openrouter/minimax/minimax-m2.1",
    "provider": "openrouter",
    "input": 2.7E-7,
    "output": 0.0000012,
    "max_input_tokens": 204000,
    "max_output_tokens": 64000
  },
  {
    "model": "ovhcloud/DeepSeek-R1-Distill-Llama-70B",
    "provider": "ovhcloud",
    "input": 6.7E-7,
    "output": 6.7E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/Llama-3.1-8B-Instruct",
    "provider": "ovhcloud",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/Meta-Llama-3_1-70B-Instruct",
    "provider": "ovhcloud",
    "input": 6.7E-7,
    "output": 6.7E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/Meta-Llama-3_3-70B-Instruct",
    "provider": "ovhcloud",
    "input": 6.7E-7,
    "output": 6.7E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/Mistral-7B-Instruct-v0.3",
    "provider": "ovhcloud",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 127000,
    "max_output_tokens": 127000
  },
  {
    "model": "ovhcloud/Mistral-Nemo-Instruct-2407",
    "provider": "ovhcloud",
    "input": 1.3E-7,
    "output": 1.3E-7,
    "max_input_tokens": 118000,
    "max_output_tokens": 118000
  },
  {
    "model": "ovhcloud/Mistral-Small-3.2-24B-Instruct-2506",
    "provider": "ovhcloud",
    "input": 9E-8,
    "output": 2.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "ovhcloud/gpt-oss-120b",
    "provider": "ovhcloud",
    "input": 8E-8,
    "output": 4E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/gpt-oss-20b",
    "provider": "ovhcloud",
    "input": 4E-8,
    "output": 1.5E-7,
    "max_input_tokens": 131000,
    "max_output_tokens": 131000
  },
  {
    "model": "ovhcloud/mamba-codestral-7B-v0.1",
    "provider": "ovhcloud",
    "input": 1.9E-7,
    "output": 1.9E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "perplexity/llama-3.1-70b-instruct",
    "provider": "perplexity",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "perplexity/llama-3.1-8b-instruct",
    "provider": "perplexity",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "perplexity/llama-3.1-sonar-huge-128k-online",
    "provider": "perplexity",
    "input": 0.000005,
    "output": 0.000005,
    "max_input_tokens": 127072,
    "max_output_tokens": 127072
  },
  {
    "model": "perplexity/llama-3.1-sonar-large-128k-chat",
    "provider": "perplexity",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "perplexity/llama-3.1-sonar-large-128k-online",
    "provider": "perplexity",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 127072,
    "max_output_tokens": 127072
  },
  {
    "model": "perplexity/llama-3.1-sonar-small-128k-chat",
    "provider": "perplexity",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "perplexity/llama-3.1-sonar-small-128k-online",
    "provider": "perplexity",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 127072,
    "max_output_tokens": 127072
  },
  {
    "model": "perplexity/sonar",
    "provider": "perplexity",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "perplexity/sonar-deep-research",
    "provider": "perplexity",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "perplexity/sonar-pro",
    "provider": "perplexity",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8000
  },
  {
    "model": "perplexity/sonar-reasoning",
    "provider": "perplexity",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "perplexity/sonar-reasoning-pro",
    "provider": "perplexity",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "qwen.qwen3-coder-480b-a35b-v1:0",
    "provider": "bedrock_converse",
    "input": 2.2E-7,
    "output": 0.0000018,
    "max_input_tokens": 262000,
    "max_output_tokens": 65536
  },
  {
    "model": "qwen.qwen3-235b-a22b-2507-v1:0",
    "provider": "bedrock_converse",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 131072
  },
  {
    "model": "qwen.qwen3-coder-30b-a3b-v1:0",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 131072
  },
  {
    "model": "qwen.qwen3-32b-v1:0",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "qwen.qwen3-next-80b-a3b",
    "provider": "bedrock_converse",
    "input": 1.5E-7,
    "output": 0.0000012,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "qwen.qwen3-vl-235b-a22b",
    "provider": "bedrock_converse",
    "input": 5.3E-7,
    "output": 0.00000266,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "qwen.qwen3-coder-next",
    "provider": "bedrock_converse",
    "input": 5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 8192
  },
  {
    "model": "replicate/deepseek-ai/deepseek-v3.1",
    "provider": "replicate",
    "input": 6.72E-7,
    "output": 0.000002016,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "sambanova/DeepSeek-R1-Distill-Llama-70B",
    "provider": "sambanova",
    "input": 7E-7,
    "output": 0.0000014,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "sambanova/Llama-4-Maverick-17B-128E-Instruct",
    "provider": "sambanova",
    "input": 6.3E-7,
    "output": 0.0000018,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "sambanova/Meta-Llama-3.3-70B-Instruct",
    "provider": "sambanova",
    "input": 6E-7,
    "output": 0.0000012,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "sambanova/gpt-oss-120b",
    "provider": "sambanova",
    "input": 0.000003,
    "output": 0.0000045,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput",
    "provider": "together_ai",
    "input": 2E-7,
    "output": 0.000006,
    "max_input_tokens": 262000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507",
    "provider": "together_ai",
    "input": 6.5E-7,
    "output": 0.000003,
    "max_input_tokens": 256000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8",
    "provider": "together_ai",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 256000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/deepseek-ai/DeepSeek-R1",
    "provider": "together_ai",
    "input": 0.000003,
    "output": 0.000007,
    "max_input_tokens": 128000,
    "max_output_tokens": 20480
  },
  {
    "model": "together_ai/deepseek-ai/DeepSeek-R1-0528-tput",
    "provider": "together_ai",
    "input": 5.5E-7,
    "output": 0.00000219,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/openai/gpt-oss-120b",
    "provider": "together_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/openai/gpt-oss-20b",
    "provider": "together_ai",
    "input": 5E-8,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/zai-org/GLM-4.5-Air-FP8",
    "provider": "together_ai",
    "input": 2E-7,
    "output": 0.0000011,
    "max_input_tokens": 128000,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/zai-org/GLM-4.6",
    "provider": "together_ai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "together_ai/zai-org/GLM-4.7",
    "provider": "together_ai",
    "input": 4.5E-7,
    "output": 0.000002,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "together_ai/moonshotai/Kimi-K2.5",
    "provider": "together_ai",
    "input": 5E-7,
    "output": 0.0000028,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "together_ai/moonshotai/Kimi-K2-Instruct-0905",
    "provider": "together_ai",
    "input": 0.000001,
    "output": 0.000003,
    "max_input_tokens": 262144,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/Qwen/Qwen3-Next-80B-A3B-Instruct",
    "provider": "together_ai",
    "input": 1.5E-7,
    "output": 0.0000015,
    "max_input_tokens": 262144,
    "max_output_tokens": null
  },
  {
    "model": "together_ai/Qwen/Qwen3-Next-80B-A3B-Thinking",
    "provider": "together_ai",
    "input": 1.5E-7,
    "output": 0.0000015,
    "max_input_tokens": 262144,
    "max_output_tokens": null
  },
  {
    "model": "us.amazon.nova-lite-v1:0",
    "provider": "bedrock_converse",
    "input": 6E-8,
    "output": 2.4E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "us.amazon.nova-micro-v1:0",
    "provider": "bedrock_converse",
    "input": 3.5E-8,
    "output": 1.4E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 10000
  },
  {
    "model": "us.amazon.nova-premier-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000025,
    "output": 0.0000125,
    "max_input_tokens": 1000000,
    "max_output_tokens": 10000
  },
  {
    "model": "us.amazon.nova-pro-v1:0",
    "provider": "bedrock_converse",
    "input": 8E-7,
    "output": 0.0000032,
    "max_input_tokens": 300000,
    "max_output_tokens": 10000
  },
  {
    "model": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
    "provider": "bedrock",
    "input": 8E-7,
    "output": 0.000004,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "us.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000011,
    "output": 0.0000055,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.anthropic.claude-3-5-sonnet-20240620-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "us.anthropic.claude-3-haiku-20240307-v1:0",
    "provider": "bedrock",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.anthropic.claude-3-opus-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.anthropic.claude-3-sonnet-20240229-v1:0",
    "provider": "bedrock",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.anthropic.claude-opus-4-1-20250805-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "us.anthropic.claude-sonnet-4-5-20250929-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000033,
    "output": 0.0000165,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "au.anthropic.claude-haiku-4-5-20251001-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000011,
    "output": 0.0000055,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.anthropic.claude-opus-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "us.anthropic.claude-opus-4-5-20251101-v1:0",
    "provider": "bedrock_converse",
    "input": 0.0000055,
    "output": 0.0000275,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "global.anthropic.claude-opus-4-5-20251101-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "eu.anthropic.claude-opus-4-5-20251101-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.anthropic.claude-sonnet-4-20250514-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "us.deepseek.r1-v1:0",
    "provider": "bedrock_converse",
    "input": 0.00000135,
    "output": 0.0000054,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.deepseek.v3.2",
    "provider": "bedrock_converse",
    "input": 6.2E-7,
    "output": 0.00000185,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "eu.deepseek.v3.2",
    "provider": "bedrock_converse",
    "input": 7.4E-7,
    "output": 0.00000222,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "us.meta.llama3-1-405b-instruct-v1:0",
    "provider": "bedrock",
    "input": 0.00000532,
    "output": 0.000016,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama3-1-70b-instruct-v1:0",
    "provider": "bedrock",
    "input": 9.9E-7,
    "output": 9.9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "us.meta.llama3-1-8b-instruct-v1:0",
    "provider": "bedrock",
    "input": 2.2E-7,
    "output": 2.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "us.meta.llama3-2-11b-instruct-v1:0",
    "provider": "bedrock",
    "input": 3.5E-7,
    "output": 3.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama3-2-1b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama3-2-3b-instruct-v1:0",
    "provider": "bedrock",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama3-2-90b-instruct-v1:0",
    "provider": "bedrock",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama3-3-70b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama4-maverick-17b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 2.4E-7,
    "output": 9.7E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.meta.llama4-scout-17b-instruct-v1:0",
    "provider": "bedrock_converse",
    "input": 1.7E-7,
    "output": 6.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "us.mistral.pixtral-large-2502-v1:0",
    "provider": "bedrock_converse",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "v0/v0-1.0-md",
    "provider": "v0",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "v0/v0-1.5-lg",
    "provider": "v0",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 512000,
    "max_output_tokens": 512000
  },
  {
    "model": "v0/v0-1.5-md",
    "provider": "v0",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vercel_ai_gateway/alibaba/qwen3-coder",
    "provider": "vercel_ai_gateway",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 262144,
    "max_output_tokens": 66536
  },
  {
    "model": "vercel_ai_gateway/amazon/nova-lite",
    "provider": "vercel_ai_gateway",
    "input": 6E-8,
    "output": 2.4E-7,
    "max_input_tokens": 300000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/amazon/nova-micro",
    "provider": "vercel_ai_gateway",
    "input": 3.5E-8,
    "output": 1.4E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/amazon/nova-pro",
    "provider": "vercel_ai_gateway",
    "input": 8E-7,
    "output": 0.0000032,
    "max_input_tokens": 300000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3-haiku",
    "provider": "vercel_ai_gateway",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3-opus",
    "provider": "vercel_ai_gateway",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3.5-haiku",
    "provider": "vercel_ai_gateway",
    "input": 8E-7,
    "output": 0.000004,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3.5-sonnet",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3.7-sonnet",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-4-opus",
    "provider": "vercel_ai_gateway",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-4-sonnet",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3-5-sonnet",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3-5-sonnet-20241022",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-3-7-sonnet",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-haiku-4.5",
    "provider": "vercel_ai_gateway",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-opus-4",
    "provider": "vercel_ai_gateway",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-opus-4.1",
    "provider": "vercel_ai_gateway",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-opus-4.5",
    "provider": "vercel_ai_gateway",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-opus-4.6",
    "provider": "vercel_ai_gateway",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-sonnet-4",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/anthropic/claude-sonnet-4.5",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/cohere/command-a",
    "provider": "vercel_ai_gateway",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 256000,
    "max_output_tokens": 8000
  },
  {
    "model": "vercel_ai_gateway/cohere/command-r",
    "provider": "vercel_ai_gateway",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "vercel_ai_gateway/cohere/command-r-plus",
    "provider": "vercel_ai_gateway",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "vercel_ai_gateway/deepseek/deepseek-r1",
    "provider": "vercel_ai_gateway",
    "input": 5.5E-7,
    "output": 0.00000219,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/deepseek/deepseek-r1-distill-llama-70b",
    "provider": "vercel_ai_gateway",
    "input": 7.5E-7,
    "output": 9.9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/deepseek/deepseek-v3",
    "provider": "vercel_ai_gateway",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/google/gemini-2.0-flash",
    "provider": "vercel_ai_gateway",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/google/gemini-2.0-flash-lite",
    "provider": "vercel_ai_gateway",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/google/gemini-2.5-flash",
    "provider": "vercel_ai_gateway",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 65536
  },
  {
    "model": "vercel_ai_gateway/google/gemini-2.5-pro",
    "provider": "vercel_ai_gateway",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65536
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.1-70b",
    "provider": "vercel_ai_gateway",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.1-8b",
    "provider": "vercel_ai_gateway",
    "input": 5E-8,
    "output": 8E-8,
    "max_input_tokens": 131000,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.2-11b",
    "provider": "vercel_ai_gateway",
    "input": 1.6E-7,
    "output": 1.6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.2-1b",
    "provider": "vercel_ai_gateway",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.2-3b",
    "provider": "vercel_ai_gateway",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.2-90b",
    "provider": "vercel_ai_gateway",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-3.3-70b",
    "provider": "vercel_ai_gateway",
    "input": 7.2E-7,
    "output": 7.2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-4-maverick",
    "provider": "vercel_ai_gateway",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/meta/llama-4-scout",
    "provider": "vercel_ai_gateway",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "vercel_ai_gateway/mistral/codestral",
    "provider": "vercel_ai_gateway",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/mistral/devstral-small",
    "provider": "vercel_ai_gateway",
    "input": 7E-8,
    "output": 2.8E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vercel_ai_gateway/mistral/magistral-medium",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.000005,
    "max_input_tokens": 128000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/mistral/magistral-small",
    "provider": "vercel_ai_gateway",
    "input": 5E-7,
    "output": 0.0000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 64000
  },
  {
    "model": "vercel_ai_gateway/mistral/ministral-3b",
    "provider": "vercel_ai_gateway",
    "input": 4E-8,
    "output": 4E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/mistral/ministral-8b",
    "provider": "vercel_ai_gateway",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/mistral/pixtral-12b",
    "provider": "vercel_ai_gateway",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/mistral/pixtral-large",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/moonshotai/kimi-k2",
    "provider": "vercel_ai_gateway",
    "input": 5.5E-7,
    "output": 0.0000022,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4-turbo",
    "provider": "vercel_ai_gateway",
    "input": 0.00001,
    "output": 0.00003,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4.1",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4.1-mini",
    "provider": "vercel_ai_gateway",
    "input": 4E-7,
    "output": 0.0000016,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4.1-nano",
    "provider": "vercel_ai_gateway",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1047576,
    "max_output_tokens": 32768
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4o",
    "provider": "vercel_ai_gateway",
    "input": 0.0000025,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "vercel_ai_gateway/openai/gpt-4o-mini",
    "provider": "vercel_ai_gateway",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 16384
  },
  {
    "model": "vercel_ai_gateway/openai/o1",
    "provider": "vercel_ai_gateway",
    "input": 0.000015,
    "output": 0.00006,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "vercel_ai_gateway/openai/o3",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "vercel_ai_gateway/openai/o3-mini",
    "provider": "vercel_ai_gateway",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "vercel_ai_gateway/openai/o4-mini",
    "provider": "vercel_ai_gateway",
    "input": 0.0000011,
    "output": 0.0000044,
    "max_input_tokens": 200000,
    "max_output_tokens": 100000
  },
  {
    "model": "vercel_ai_gateway/perplexity/sonar",
    "provider": "vercel_ai_gateway",
    "input": 0.000001,
    "output": 0.000001,
    "max_input_tokens": 127000,
    "max_output_tokens": 8000
  },
  {
    "model": "vercel_ai_gateway/perplexity/sonar-pro",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8000
  },
  {
    "model": "vercel_ai_gateway/perplexity/sonar-reasoning",
    "provider": "vercel_ai_gateway",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 127000,
    "max_output_tokens": 8000
  },
  {
    "model": "vercel_ai_gateway/perplexity/sonar-reasoning-pro",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 127000,
    "max_output_tokens": 8000
  },
  {
    "model": "vercel_ai_gateway/vercel/v0-1.0-md",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "vercel_ai_gateway/vercel/v0-1.5-md",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 128000,
    "max_output_tokens": 32768
  },
  {
    "model": "vercel_ai_gateway/xai/grok-2",
    "provider": "vercel_ai_gateway",
    "input": 0.000002,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 4000
  },
  {
    "model": "vercel_ai_gateway/xai/grok-3",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/xai/grok-3-fast",
    "provider": "vercel_ai_gateway",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/xai/grok-3-mini",
    "provider": "vercel_ai_gateway",
    "input": 3E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/xai/grok-3-mini-fast",
    "provider": "vercel_ai_gateway",
    "input": 6E-7,
    "output": 0.000004,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/xai/grok-4",
    "provider": "vercel_ai_gateway",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vercel_ai_gateway/zai/glm-4.5",
    "provider": "vercel_ai_gateway",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "vercel_ai_gateway/zai/glm-4.5-air",
    "provider": "vercel_ai_gateway",
    "input": 2E-7,
    "output": 0.0000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 96000
  },
  {
    "model": "vercel_ai_gateway/zai/glm-4.6",
    "provider": "vercel_ai_gateway",
    "input": 4.5E-7,
    "output": 0.0000018,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "vertex_ai/claude-3-5-haiku",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-5-haiku@20241022",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-haiku-4-5@20251001",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000001,
    "output": 0.000005,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-5-sonnet",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-5-sonnet-v2",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-5-sonnet-v2@20241022",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-5-sonnet@20240620",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-7-sonnet@20250219",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 8192
  },
  {
    "model": "vertex_ai/claude-3-haiku",
    "provider": "vertex_ai-anthropic_models",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-3-haiku@20240307",
    "provider": "vertex_ai-anthropic_models",
    "input": 2.5E-7,
    "output": 0.00000125,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-3-opus",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-3-opus@20240229",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-3-sonnet",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-3-sonnet@20240229",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 4096
  },
  {
    "model": "vertex_ai/claude-opus-4",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vertex_ai/claude-opus-4-1",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vertex_ai/claude-opus-4-1@20250805",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vertex_ai/claude-opus-4-5",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-opus-4-5@20251101",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-opus-4-6",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/claude-opus-4-6@default",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 1000000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/claude-sonnet-4-5",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-sonnet-4-6",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-sonnet-4-5@20250929",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-opus-4@20250514",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000015,
    "output": 0.000075,
    "max_input_tokens": 200000,
    "max_output_tokens": 32000
  },
  {
    "model": "vertex_ai/claude-sonnet-4",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/claude-sonnet-4@20250514",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 1000000,
    "max_output_tokens": 64000
  },
  {
    "model": "vertex_ai/mistralai/codestral-2@001",
    "provider": "vertex_ai-mistral_models",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/codestral-2",
    "provider": "vertex_ai-mistral_models",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/codestral-2@001",
    "provider": "vertex_ai-mistral_models",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/mistralai/codestral-2",
    "provider": "vertex_ai-mistral_models",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/codestral-2501",
    "provider": "vertex_ai-mistral_models",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/codestral@2405",
    "provider": "vertex_ai-mistral_models",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/codestral@latest",
    "provider": "vertex_ai-mistral_models",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/deepseek-ai/deepseek-v3.1-maas",
    "provider": "vertex_ai-deepseek_models",
    "input": 0.00000135,
    "output": 0.0000054,
    "max_input_tokens": 163840,
    "max_output_tokens": 32768
  },
  {
    "model": "vertex_ai/deepseek-ai/deepseek-v3.2-maas",
    "provider": "vertex_ai-deepseek_models",
    "input": 5.6E-7,
    "output": 0.00000168,
    "max_input_tokens": 163840,
    "max_output_tokens": 32768
  },
  {
    "model": "vertex_ai/jamba-1.5",
    "provider": "vertex_ai-ai21_models",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/jamba-1.5-large",
    "provider": "vertex_ai-ai21_models",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/jamba-1.5-large@001",
    "provider": "vertex_ai-ai21_models",
    "input": 0.000002,
    "output": 0.000008,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/jamba-1.5-mini",
    "provider": "vertex_ai-ai21_models",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/jamba-1.5-mini@001",
    "provider": "vertex_ai-ai21_models",
    "input": 2E-7,
    "output": 4E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/meta/llama-3.1-405b-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 0.000005,
    "output": 0.000016,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "vertex_ai/meta/llama-3.1-70b-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "vertex_ai/meta/llama-3.1-8b-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "vertex_ai/meta/llama-3.2-90b-vision-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 0.0,
    "output": 0.0,
    "max_input_tokens": 128000,
    "max_output_tokens": 2048
  },
  {
    "model": "vertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 3.5E-7,
    "output": 0.00000115,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "vertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 3.5E-7,
    "output": 0.00000115,
    "max_input_tokens": 1000000,
    "max_output_tokens": 1000000
  },
  {
    "model": "vertex_ai/meta/llama-4-scout-17b-128e-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 2.5E-7,
    "output": 7E-7,
    "max_input_tokens": 10000000,
    "max_output_tokens": 10000000
  },
  {
    "model": "vertex_ai/meta/llama-4-scout-17b-16e-instruct-maas",
    "provider": "vertex_ai-llama_models",
    "input": 2.5E-7,
    "output": 7E-7,
    "max_input_tokens": 10000000,
    "max_output_tokens": 10000000
  },
  {
    "model": "vertex_ai/minimaxai/minimax-m2-maas",
    "provider": "vertex_ai-minimax_models",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 196608,
    "max_output_tokens": 196608
  },
  {
    "model": "vertex_ai/moonshotai/kimi-k2-thinking-maas",
    "provider": "vertex_ai-moonshot_models",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "vertex_ai/zai-org/glm-4.7-maas",
    "provider": "vertex_ai-zai_models",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/zai-org/glm-5-maas",
    "provider": "vertex_ai-zai_models",
    "input": 0.000001,
    "output": 0.0000032,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/mistral-medium-3",
    "provider": "vertex_ai-mistral_models",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-medium-3@001",
    "provider": "vertex_ai-mistral_models",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistralai/mistral-medium-3",
    "provider": "vertex_ai-mistral_models",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistralai/mistral-medium-3@001",
    "provider": "vertex_ai-mistral_models",
    "input": 4E-7,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-large-2411",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-large@2407",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-large@2411-001",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-large@latest",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000002,
    "output": 0.000006,
    "max_input_tokens": 128000,
    "max_output_tokens": 8191
  },
  {
    "model": "vertex_ai/mistral-nemo@2407",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000003,
    "output": 0.000003,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/mistral-nemo@latest",
    "provider": "vertex_ai-mistral_models",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/mistral-small-2503",
    "provider": "vertex_ai-mistral_models",
    "input": 0.000001,
    "output": 0.000003,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "vertex_ai/openai/gpt-oss-120b-maas",
    "provider": "vertex_ai-openai_models",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "vertex_ai/openai/gpt-oss-20b-maas",
    "provider": "vertex_ai-openai_models",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "vertex_ai/qwen/qwen3-235b-a22b-instruct-2507-maas",
    "provider": "vertex_ai-qwen_models",
    "input": 2.5E-7,
    "output": 0.000001,
    "max_input_tokens": 262144,
    "max_output_tokens": 16384
  },
  {
    "model": "vertex_ai/qwen/qwen3-coder-480b-a35b-instruct-maas",
    "provider": "vertex_ai-qwen_models",
    "input": 0.000001,
    "output": 0.000004,
    "max_input_tokens": 262144,
    "max_output_tokens": 32768
  },
  {
    "model": "vertex_ai/qwen/qwen3-next-80b-a3b-instruct-maas",
    "provider": "vertex_ai-qwen_models",
    "input": 1.5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "vertex_ai/qwen/qwen3-next-80b-a3b-thinking-maas",
    "provider": "vertex_ai-qwen_models",
    "input": 1.5E-7,
    "output": 0.0000012,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "voyage/voyage-context-3",
    "provider": "voyage",
    "input": 1.8E-7,
    "output": 0.0,
    "max_input_tokens": 120000,
    "max_output_tokens": null
  },
  {
    "model": "wandb/openai/gpt-oss-120b",
    "provider": "wandb",
    "input": 0.015,
    "output": 0.06,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "wandb/openai/gpt-oss-20b",
    "provider": "wandb",
    "input": 0.005,
    "output": 0.02,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "wandb/zai-org/GLM-4.5",
    "provider": "wandb",
    "input": 0.055,
    "output": 0.2,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "wandb/Qwen/Qwen3-235B-A22B-Instruct-2507",
    "provider": "wandb",
    "input": 0.01,
    "output": 0.01,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "wandb/Qwen/Qwen3-Coder-480B-A35B-Instruct",
    "provider": "wandb",
    "input": 0.1,
    "output": 0.15,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "wandb/Qwen/Qwen3-235B-A22B-Thinking-2507",
    "provider": "wandb",
    "input": 0.01,
    "output": 0.01,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "wandb/moonshotai/Kimi-K2-Instruct",
    "provider": "wandb",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "wandb/meta-llama/Llama-3.1-8B-Instruct",
    "provider": "wandb",
    "input": 0.022,
    "output": 0.022,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "wandb/deepseek-ai/DeepSeek-V3.1",
    "provider": "wandb",
    "input": 0.055,
    "output": 0.165,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "wandb/deepseek-ai/DeepSeek-R1-0528",
    "provider": "wandb",
    "input": 0.135,
    "output": 0.54,
    "max_input_tokens": 161000,
    "max_output_tokens": 161000
  },
  {
    "model": "wandb/deepseek-ai/DeepSeek-V3-0324",
    "provider": "wandb",
    "input": 0.114,
    "output": 0.275,
    "max_input_tokens": 161000,
    "max_output_tokens": 161000
  },
  {
    "model": "wandb/meta-llama/Llama-3.3-70B-Instruct",
    "provider": "wandb",
    "input": 0.071,
    "output": 0.071,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "wandb/microsoft/Phi-4-mini-instruct",
    "provider": "wandb",
    "input": 0.008,
    "output": 0.035,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/mistralai/mistral-large",
    "provider": "watsonx",
    "input": 0.000003,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "watsonx/meta-llama/llama-3-2-11b-vision-instruct",
    "provider": "watsonx",
    "input": 3.5E-7,
    "output": 3.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-3-2-1b-instruct",
    "provider": "watsonx",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-3-2-3b-instruct",
    "provider": "watsonx",
    "input": 1.5E-7,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-3-2-90b-vision-instruct",
    "provider": "watsonx",
    "input": 0.000002,
    "output": 0.000002,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-3-3-70b-instruct",
    "provider": "watsonx",
    "input": 7.1E-7,
    "output": 7.1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-4-maverick-17b",
    "provider": "watsonx",
    "input": 3.5E-7,
    "output": 0.0000014,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/meta-llama/llama-guard-3-11b-vision",
    "provider": "watsonx",
    "input": 3.5E-7,
    "output": 3.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/mistralai/mistral-medium-2505",
    "provider": "watsonx",
    "input": 0.000003,
    "output": 0.00001,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "watsonx/mistralai/pixtral-12b-2409",
    "provider": "watsonx",
    "input": 3.5E-7,
    "output": 3.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "xai/grok-2",
    "provider": "xai",
    "input": 0.000002,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-2-1212",
    "provider": "xai",
    "input": 0.000002,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-2-latest",
    "provider": "xai",
    "input": 0.000002,
    "output": 0.00001,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-beta",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-fast-beta",
    "provider": "xai",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-fast-latest",
    "provider": "xai",
    "input": 0.000005,
    "output": 0.000025,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-latest",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini",
    "provider": "xai",
    "input": 3E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini-beta",
    "provider": "xai",
    "input": 3E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini-fast",
    "provider": "xai",
    "input": 6E-7,
    "output": 0.000004,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini-fast-beta",
    "provider": "xai",
    "input": 6E-7,
    "output": 0.000004,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini-fast-latest",
    "provider": "xai",
    "input": 6E-7,
    "output": 0.000004,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-3-mini-latest",
    "provider": "xai",
    "input": 3E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-4",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "xai/grok-4-fast-reasoning",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-fast-non-reasoning",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-0709",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "xai/grok-4-latest",
    "provider": "xai",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "xai/grok-4-1-fast",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-1-fast-reasoning",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-1-fast-reasoning-latest",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-1-fast-non-reasoning",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-4-1-fast-non-reasoning-latest",
    "provider": "xai",
    "input": 2E-7,
    "output": 5E-7,
    "max_input_tokens": 2000000.0,
    "max_output_tokens": 2000000.0
  },
  {
    "model": "xai/grok-beta",
    "provider": "xai",
    "input": 0.000005,
    "output": 0.000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "xai/grok-code-fast",
    "provider": "xai",
    "input": 2E-7,
    "output": 0.0000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "xai/grok-code-fast-1",
    "provider": "xai",
    "input": 2E-7,
    "output": 0.0000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "xai/grok-code-fast-1-0825",
    "provider": "xai",
    "input": 2E-7,
    "output": 0.0000015,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "zai.glm-4.7",
    "provider": "bedrock_converse",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "zai/glm-4.7",
    "provider": "zai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "zai/glm-4.6",
    "provider": "zai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 200000,
    "max_output_tokens": 128000
  },
  {
    "model": "zai/glm-4.5",
    "provider": "zai",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4.5v",
    "provider": "zai",
    "input": 6E-7,
    "output": 0.0000018,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4.5-x",
    "provider": "zai",
    "input": 0.0000022,
    "output": 0.0000089,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4.5-air",
    "provider": "zai",
    "input": 2E-7,
    "output": 0.0000011,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4.5-airx",
    "provider": "zai",
    "input": 0.0000011,
    "output": 0.0000045,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4-32b-0414-128k",
    "provider": "zai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "zai/glm-4.5-flash",
    "provider": "zai",
    "input": 0,
    "output": 0,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-coder-480b-a35b-instruct",
    "provider": "fireworks_ai",
    "input": 4.5E-7,
    "output": 0.0000018,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-671b-v2-p1",
    "provider": "fireworks_ai",
    "input": 0.0000012,
    "output": 0.0000012,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-3b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-70b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-v1-preview-llama-8b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-14b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/cogito-v1-preview-qwen-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-base",
    "provider": "fireworks_ai",
    "input": 5E-7,
    "output": 5E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-coder-v2-lite-instruct",
    "provider": "fireworks_ai",
    "input": 5E-7,
    "output": 5E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-prover-v2",
    "provider": "fireworks_ai",
    "input": 0.0000012,
    "output": 0.0000012,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-0528-distill-qwen3-8b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-70b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-llama-8b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-14b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-1p5b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-distill-qwen-7b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/deepseek-v2-lite-chat",
    "provider": "fireworks_ai",
    "input": 5E-7,
    "output": 5E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/devstral-small-2505",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/dobby-mini-unhinged-plus-llama-3-1-8b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/dobby-unhinged-llama-3-3-70b-new",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/dolphin-2-9-2-qwen2-72b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/fare-20b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/gemma-3-27b-it",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/glm-4p5v",
    "provider": "fireworks_ai",
    "input": 0.0000012,
    "output": 0.0000012,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-120b",
    "provider": "fireworks_ai",
    "input": 0.0000012,
    "output": 0.0000012,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/gpt-oss-safeguard-20b",
    "provider": "fireworks_ai",
    "input": 5E-7,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kat-coder",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kat-dev-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/kat-dev-72b-exp",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-guard-3-1b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-guard-3-8b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p1-nemotron-70b-instruct",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p2-1b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p2-3b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/llama-v3p3-70b-instruct",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/ministral-3-14b-instruct-2512",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/ministral-3-3b-instruct-2512",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/ministral-3-8b-instruct-2512",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/mistral-large-3-fp8",
    "provider": "fireworks_ai",
    "input": 0.0000012,
    "output": 0.0000012,
    "max_input_tokens": 256000,
    "max_output_tokens": 256000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/mistral-nemo-base-2407",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/mistral-nemo-instruct-2407",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-12b-v2",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/nvidia-nemotron-nano-9b-v2",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/phi-3-mini-128k-instruct",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen-v2p5-7b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-14b",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-72b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-coder-32b-instruct-128k",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-vl-32b-instruct",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-vl-3b-instruct",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-vl-72b-instruct",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen2p5-vl-7b-instruct",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-1p7b",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-1p7b-fp8-draft-131072",
    "provider": "fireworks_ai",
    "input": 1E-7,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-instruct-2507",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-235b-a22b-thinking-2507",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-instruct-2507",
    "provider": "fireworks_ai",
    "input": 5E-7,
    "output": 5E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-30b-a3b-thinking-2507",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-4b-instruct-2507",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-coder-30b-a3b-instruct",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-instruct",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-235b-a22b-thinking",
    "provider": "fireworks_ai",
    "input": 2.2E-7,
    "output": 8.8E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-instruct",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwen3-vl-30b-a3b-thinking",
    "provider": "fireworks_ai",
    "input": 1.5E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/qwq-32b",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/rolm-ocr",
    "provider": "fireworks_ai",
    "input": 2E-7,
    "output": 2E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 128000
  },
  {
    "model": "fireworks_ai/accounts/fireworks/models/yi-34b-200k-capybara",
    "provider": "fireworks_ai",
    "input": 9E-7,
    "output": 9E-7,
    "max_input_tokens": 200000,
    "max_output_tokens": 200000
  },
  {
    "model": "novita/deepseek/deepseek-v3.2",
    "provider": "novita",
    "input": 2.69E-7,
    "output": 4E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/minimax/minimax-m2.1",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 204800,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/zai-org/glm-4.7",
    "provider": "novita",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 204800,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/xiaomimimo/mimo-v2-flash",
    "provider": "novita",
    "input": 1E-7,
    "output": 3E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 32000
  },
  {
    "model": "novita/moonshotai/kimi-k2-thinking",
    "provider": "novita",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "novita/minimax/minimax-m2",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 204800,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/deepseek/deepseek-v3.2-exp",
    "provider": "novita",
    "input": 2.7E-7,
    "output": 4.1E-7,
    "max_input_tokens": 163840,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/qwen/qwen3-vl-235b-a22b-thinking",
    "provider": "novita",
    "input": 9.8E-7,
    "output": 0.00000395,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/zai-org/glm-4.6v",
    "provider": "novita",
    "input": 3E-7,
    "output": 9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/zai-org/glm-4.6",
    "provider": "novita",
    "input": 5.5E-7,
    "output": 0.0000022,
    "max_input_tokens": 204800,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/kwaipilot/kat-coder-pro",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.0000012,
    "max_input_tokens": 256000,
    "max_output_tokens": 128000
  },
  {
    "model": "novita/qwen/qwen3-next-80b-a3b-instruct",
    "provider": "novita",
    "input": 1.5E-7,
    "output": 0.0000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/qwen/qwen3-next-80b-a3b-thinking",
    "provider": "novita",
    "input": 1.5E-7,
    "output": 0.0000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/deepseek/deepseek-v3.1-terminus",
    "provider": "novita",
    "input": 2.7E-7,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/qwen/qwen3-vl-235b-a22b-instruct",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.0000015,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/qwen/qwen3-max",
    "provider": "novita",
    "input": 0.00000211,
    "output": 0.00000845,
    "max_input_tokens": 262144,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/skywork/r1v4-lite",
    "provider": "novita",
    "input": 2E-7,
    "output": 6E-7,
    "max_input_tokens": 262144,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/deepseek/deepseek-v3.1",
    "provider": "novita",
    "input": 2.7E-7,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/moonshotai/kimi-k2-0905",
    "provider": "novita",
    "input": 6E-7,
    "output": 0.0000025,
    "max_input_tokens": 262144,
    "max_output_tokens": 262144
  },
  {
    "model": "novita/qwen/qwen3-coder-480b-a35b-instruct",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.0000013,
    "max_input_tokens": 262144,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/qwen/qwen3-coder-30b-a3b-instruct",
    "provider": "novita",
    "input": 7E-8,
    "output": 2.7E-7,
    "max_input_tokens": 160000,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/openai/gpt-oss-120b",
    "provider": "novita",
    "input": 5E-8,
    "output": 2.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/moonshotai/kimi-k2-instruct",
    "provider": "novita",
    "input": 5.7E-7,
    "output": 0.0000023,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/deepseek/deepseek-v3-0324",
    "provider": "novita",
    "input": 2.7E-7,
    "output": 0.00000112,
    "max_input_tokens": 163840,
    "max_output_tokens": 163840
  },
  {
    "model": "novita/zai-org/glm-4.5",
    "provider": "novita",
    "input": 6E-7,
    "output": 0.0000022,
    "max_input_tokens": 131072,
    "max_output_tokens": 98304
  },
  {
    "model": "novita/qwen/qwen3-235b-a22b-thinking-2507",
    "provider": "novita",
    "input": 3E-7,
    "output": 0.000003,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/google/gemma-3-12b-it",
    "provider": "novita",
    "input": 5E-8,
    "output": 1E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "novita/openai/gpt-oss-20b",
    "provider": "novita",
    "input": 4E-8,
    "output": 1.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/qwen/qwen3-235b-a22b-instruct-2507",
    "provider": "novita",
    "input": 9E-8,
    "output": 5.8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "novita/meta-llama/llama-3.3-70b-instruct",
    "provider": "novita",
    "input": 1.35E-7,
    "output": 4E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 120000
  },
  {
    "model": "novita/minimaxai/minimax-m1-80k",
    "provider": "novita",
    "input": 5.5E-7,
    "output": 0.0000022,
    "max_input_tokens": 1000000,
    "max_output_tokens": 40000
  },
  {
    "model": "novita/deepseek/deepseek-r1-0528",
    "provider": "novita",
    "input": 7E-7,
    "output": 0.0000025,
    "max_input_tokens": 163840,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/deepseek/deepseek-r1-0528-qwen3-8b",
    "provider": "novita",
    "input": 6E-8,
    "output": 9E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 32000
  },
  {
    "model": "novita/meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
    "provider": "novita",
    "input": 2.7E-7,
    "output": 8.5E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "novita/meta-llama/llama-4-scout-17b-16e-instruct",
    "provider": "novita",
    "input": 1.8E-7,
    "output": 5.9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/baidu/ernie-4.5-21B-a3b-thinking",
    "provider": "novita",
    "input": 7E-8,
    "output": 2.8E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/baichuan/baichuan-m2-32b",
    "provider": "novita",
    "input": 7E-8,
    "output": 7E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 131072
  },
  {
    "model": "novita/baidu/ernie-4.5-vl-424b-a47b",
    "provider": "novita",
    "input": 4.2E-7,
    "output": 0.00000125,
    "max_input_tokens": 123000,
    "max_output_tokens": 16000
  },
  {
    "model": "novita/baidu/ernie-4.5-300b-a47b-paddle",
    "provider": "novita",
    "input": 2.8E-7,
    "output": 0.0000011,
    "max_input_tokens": 123000,
    "max_output_tokens": 12000
  },
  {
    "model": "novita/deepseek/deepseek-prover-v2-671b",
    "provider": "novita",
    "input": 7E-7,
    "output": 0.0000025,
    "max_input_tokens": 160000,
    "max_output_tokens": 160000
  },
  {
    "model": "novita/baidu/ernie-4.5-vl-28b-a3b-thinking",
    "provider": "novita",
    "input": 3.9E-7,
    "output": 3.9E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 65536
  },
  {
    "model": "novita/qwen/qwen3-vl-8b-instruct",
    "provider": "novita",
    "input": 8E-8,
    "output": 5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/zai-org/glm-4.5-air",
    "provider": "novita",
    "input": 1.3E-7,
    "output": 8.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 98304
  },
  {
    "model": "novita/qwen/qwen3-vl-30b-a3b-instruct",
    "provider": "novita",
    "input": 2E-7,
    "output": 7E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/qwen/qwen3-vl-30b-a3b-thinking",
    "provider": "novita",
    "input": 2E-7,
    "output": 0.000001,
    "max_input_tokens": 131072,
    "max_output_tokens": 32768
  },
  {
    "model": "novita/baidu/ernie-4.5-21B-a3b",
    "provider": "novita",
    "input": 7E-8,
    "output": 2.8E-7,
    "max_input_tokens": 120000,
    "max_output_tokens": 8000
  },
  {
    "model": "novita/qwen/qwen3-8b-fp8",
    "provider": "novita",
    "input": 3.5E-8,
    "output": 1.38E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 20000
  },
  {
    "model": "novita/qwen/qwen3-4b-fp8",
    "provider": "novita",
    "input": 3E-8,
    "output": 3E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 20000
  },
  {
    "model": "llamagate/llama-3.1-8b",
    "provider": "llamagate",
    "input": 3E-8,
    "output": 5E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "llamagate/llama-3.2-3b",
    "provider": "llamagate",
    "input": 4E-8,
    "output": 8E-8,
    "max_input_tokens": 131072,
    "max_output_tokens": 8192
  },
  {
    "model": "llamagate/dolphin3-8b",
    "provider": "llamagate",
    "input": 8E-8,
    "output": 1.5E-7,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "llamagate/deepseek-r1-7b-qwen",
    "provider": "llamagate",
    "input": 8E-8,
    "output": 1.5E-7,
    "max_input_tokens": 131072,
    "max_output_tokens": 16384
  },
  {
    "model": "llamagate/gemma3-4b",
    "provider": "llamagate",
    "input": 3E-8,
    "output": 8E-8,
    "max_input_tokens": 128000,
    "max_output_tokens": 8192
  },
  {
    "model": "gpt-5-search-api",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-5-search-api-2025-10-14",
    "provider": "openai",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 272000,
    "max_output_tokens": 128000
  },
  {
    "model": "gpt-realtime-mini-2025-10-06",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gpt-realtime-mini-2025-12-15",
    "provider": "openai",
    "input": 6E-7,
    "output": 0.0000024,
    "max_input_tokens": 128000,
    "max_output_tokens": 4096
  },
  {
    "model": "gemini/gemini-2.0-flash-lite-001",
    "provider": "gemini",
    "input": 7.5E-8,
    "output": 3E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.5-flash-native-audio-latest",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.5-flash-native-audio-preview-09-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-2.5-flash-native-audio-preview-12-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.5-flash-native-audio-latest",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.5-flash-native-audio-preview-09-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini/gemini-2.5-flash-native-audio-preview-12-2025",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 8192
  },
  {
    "model": "gemini-flash-latest",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-flash-lite-latest",
    "provider": "gemini",
    "input": 1E-7,
    "output": 4E-7,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-pro-latest",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini/gemini-pro-latest",
    "provider": "gemini",
    "input": 0.00000125,
    "output": 0.00001,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "gemini-exp-1206",
    "provider": "gemini",
    "input": 3E-7,
    "output": 0.0000025,
    "max_input_tokens": 1048576,
    "max_output_tokens": 65535
  },
  {
    "model": "vertex_ai/claude-sonnet-4-6@default",
    "provider": "vertex_ai-anthropic_models",
    "input": 0.000003,
    "output": 0.000015,
    "max_input_tokens": 200000,
    "max_output_tokens": 64000
  }
]
