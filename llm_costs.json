[
    {
        "model": "gpt-4.1",
        "provider": "openai",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4.1-2025-04-14",
        "provider": "openai",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4.1-mini",
        "provider": "openai",
        "input": 4e-7,
        "output": 0.0000016,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4.1-mini-2025-04-14",
        "provider": "openai",
        "input": 4e-7,
        "output": 0.0000016,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4.1-nano",
        "provider": "openai",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4.1-nano-2025-04-14",
        "provider": "openai",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "gpt-4o",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-search-preview-2025-03-11",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-search-preview",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4.5-preview",
        "provider": "openai",
        "input": 0.000075,
        "output": 0.00015,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4.5-preview-2025-02-27",
        "provider": "openai",
        "input": 0.000075,
        "output": 0.00015,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-audio-preview",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-audio-preview-2024-12-17",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-audio-preview-2024-10-01",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-audio-preview-2025-06-03",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini-audio-preview",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini-audio-preview-2024-12-17",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini-search-preview-2025-03-11",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini-search-preview",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-mini-2024-07-18",
        "provider": "openai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "codex-mini-latest",
        "provider": "openai",
        "input": 0.0000015,
        "output": 0.000006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o1-pro",
        "provider": "openai",
        "input": 0.00015,
        "output": 0.0006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o1-pro-2025-03-19",
        "provider": "openai",
        "input": 0.00015,
        "output": 0.0006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o1",
        "provider": "openai",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o1-mini",
        "provider": "openai",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "o3-pro",
        "provider": "openai",
        "input": 0.00002,
        "output": 0.00008,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o3-pro-2025-06-10",
        "provider": "openai",
        "input": 0.00002,
        "output": 0.00008,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o3",
        "provider": "openai",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o3-2025-04-16",
        "provider": "openai",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o3-mini",
        "provider": "openai",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o3-mini-2025-01-31",
        "provider": "openai",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o4-mini",
        "provider": "openai",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o4-mini-2025-04-16",
        "provider": "openai",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "o1-mini-2024-09-12",
        "provider": "openai",
        "input": 0.000003,
        "output": 0.000012,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "o1-preview",
        "provider": "openai",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "o1-preview-2024-09-12",
        "provider": "openai",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "o1-2024-12-17",
        "provider": "openai",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "chatgpt-4o-latest",
        "provider": "openai",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-2024-05-13",
        "provider": "openai",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-2024-08-06",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-2024-11-20",
        "provider": "openai",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "gpt-4o-realtime-preview-2024-10-01",
        "provider": "openai",
        "input": 0.000005,
        "output": 0.00002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-realtime-preview",
        "provider": "openai",
        "input": 0.000005,
        "output": 0.00002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-realtime-preview-2024-12-17",
        "provider": "openai",
        "input": 0.000005,
        "output": 0.00002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-mini-realtime-preview",
        "provider": "openai",
        "input": 6e-7,
        "output": 0.0000024,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4o-mini-realtime-preview-2024-12-17",
        "provider": "openai",
        "input": 6e-7,
        "output": 0.0000024,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-turbo-preview",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-turbo",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-turbo-2024-04-09",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-1106-preview",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-0125-preview",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-vision-preview",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "gpt-4-1106-vision-preview",
        "provider": "openai",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "ft:gpt-4o-2024-08-06",
        "provider": "openai",
        "input": 0.00000375,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "ft:gpt-4o-2024-11-20",
        "provider": "openai",
        "input": 0.00000375,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "ft:gpt-4o-mini-2024-07-18",
        "provider": "openai",
        "input": 3e-7,
        "output": 0.0000012,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-audio-preview-2024-12-17",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-mini-audio-preview-2024-12-17",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4.1",
        "provider": "azure",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.1-2025-04-14",
        "provider": "azure",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.1-mini",
        "provider": "azure",
        "input": 4e-7,
        "output": 0.0000016,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.1-mini-2025-04-14",
        "provider": "azure",
        "input": 4e-7,
        "output": 0.0000016,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.1-nano",
        "provider": "azure",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.1-nano-2025-04-14",
        "provider": "azure",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1047576,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/o3",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00004,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o3-2025-04-16",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00004,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o4-mini",
        "provider": "azure",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/gpt-4o-mini-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 6e-7,
        "output": 0.0000024,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/eu/gpt-4o-mini-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 6.6e-7,
        "output": 0.00000264,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/us/gpt-4o-mini-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 6.6e-7,
        "output": 0.00000264,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4o-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 0.000005,
        "output": 0.00002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/us/gpt-4o-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 0.0000055,
        "output": 0.000022,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/eu/gpt-4o-realtime-preview-2024-12-17",
        "provider": "azure",
        "input": 0.0000055,
        "output": 0.000022,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4o-realtime-preview-2024-10-01",
        "provider": "azure",
        "input": 0.000005,
        "output": 0.00002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/us/gpt-4o-realtime-preview-2024-10-01",
        "provider": "azure",
        "input": 0.0000055,
        "output": 0.000022,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/eu/gpt-4o-realtime-preview-2024-10-01",
        "provider": "azure",
        "input": 0.0000055,
        "output": 0.000022,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/o4-mini-2025-04-16",
        "provider": "azure",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o3-mini-2025-01-31",
        "provider": "azure",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/us/o3-mini-2025-01-31",
        "provider": "azure",
        "input": 0.00000121,
        "output": 0.00000484,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/eu/o3-mini-2025-01-31",
        "provider": "azure",
        "input": 0.00000121,
        "output": 0.00000484,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o3-mini",
        "provider": "azure",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o1-mini",
        "provider": "azure",
        "input": 0.00000121,
        "output": 0.00000484,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "azure/o1-mini-2024-09-12",
        "provider": "azure",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "azure/us/o1-mini-2024-09-12",
        "provider": "azure",
        "input": 0.00000121,
        "output": 0.00000484,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "azure/eu/o1-mini-2024-09-12",
        "provider": "azure",
        "input": 0.00000121,
        "output": 0.00000484,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "azure/o1",
        "provider": "azure",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o1-2024-12-17",
        "provider": "azure",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/us/o1-2024-12-17",
        "provider": "azure",
        "input": 0.0000165,
        "output": 0.000066,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/eu/o1-2024-12-17",
        "provider": "azure",
        "input": 0.0000165,
        "output": 0.000066,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/codex-mini-latest",
        "provider": "azure",
        "input": 0.0000015,
        "output": 0.000006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "azure/o1-preview",
        "provider": "azure",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/o1-preview-2024-09-12",
        "provider": "azure",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/us/o1-preview-2024-09-12",
        "provider": "azure",
        "input": 0.0000165,
        "output": 0.000066,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/eu/o1-preview-2024-09-12",
        "provider": "azure",
        "input": 0.0000165,
        "output": 0.000066,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "azure/gpt-4.5-preview",
        "provider": "azure",
        "input": 0.000075,
        "output": 0.00015,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/global/gpt-4o-2024-11-20",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-2024-08-06",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/global/gpt-4o-2024-08-06",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-2024-11-20",
        "provider": "azure",
        "input": 0.00000275,
        "output": 0.000011,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/us/gpt-4o-2024-11-20",
        "provider": "azure",
        "input": 0.00000275,
        "output": 0.000011,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/eu/gpt-4o-2024-11-20",
        "provider": "azure",
        "input": 0.00000275,
        "output": 0.000011,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-2024-05-13",
        "provider": "azure",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/global-standard/gpt-4o-2024-08-06",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/us/gpt-4o-2024-08-06",
        "provider": "azure",
        "input": 0.00000275,
        "output": 0.000011,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/eu/gpt-4o-2024-08-06",
        "provider": "azure",
        "input": 0.00000275,
        "output": 0.000011,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/global-standard/gpt-4o-2024-11-20",
        "provider": "azure",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/global-standard/gpt-4o-mini",
        "provider": "azure",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-mini",
        "provider": "azure",
        "input": 1.65e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4o-mini-2024-07-18",
        "provider": "azure",
        "input": 1.65e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/us/gpt-4o-mini-2024-07-18",
        "provider": "azure",
        "input": 1.65e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/eu/gpt-4o-mini-2024-07-18",
        "provider": "azure",
        "input": 1.65e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure/gpt-4-turbo-2024-04-09",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4-0125-preview",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4-1106-preview",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4-turbo",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/gpt-4-turbo-vision-preview",
        "provider": "azure",
        "input": 0.00001,
        "output": 0.00003,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure/command-r-plus",
        "provider": "azure",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/deepseek-r1",
        "provider": "azure_ai",
        "input": 0.00000135,
        "output": 0.0000054,
        "max_input_tokens": 128000,
        "max_output_tokens": 8192
    },
    {
        "model": "azure_ai/deepseek-v3",
        "provider": "azure_ai",
        "input": 0.00000114,
        "output": 0.00000456,
        "max_input_tokens": 128000,
        "max_output_tokens": 8192
    },
    {
        "model": "azure_ai/deepseek-v3-0324",
        "provider": "azure_ai",
        "input": 0.00000114,
        "output": 0.00000456,
        "max_input_tokens": 128000,
        "max_output_tokens": 8192
    },
    {
        "model": "azure_ai/mistral-nemo",
        "provider": "azure_ai",
        "input": 1.5e-7,
        "output": 1.5e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/mistral-medium-2505",
        "provider": "azure_ai",
        "input": 4e-7,
        "output": 0.000002,
        "max_input_tokens": 131072,
        "max_output_tokens": 8191
    },
    {
        "model": "azure_ai/mistral-small-2503",
        "provider": "azure_ai",
        "input": 0.000001,
        "output": 0.000003,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "azure_ai/mistral-large-2407",
        "provider": "azure_ai",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/mistral-large-latest",
        "provider": "azure_ai",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/ministral-3b",
        "provider": "azure_ai",
        "input": 4e-8,
        "output": 4e-8,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Llama-3.2-11B-Vision-Instruct",
        "provider": "azure_ai",
        "input": 3.7e-7,
        "output": 3.7e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Llama-3.3-70B-Instruct",
        "provider": "azure_ai",
        "input": 7.1e-7,
        "output": 7.1e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Llama-4-Scout-17B-16E-Instruct",
        "provider": "azure_ai",
        "input": 2e-7,
        "output": 7.8e-7,
        "max_input_tokens": 10000000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure_ai/Llama-4-Maverick-17B-128E-Instruct-FP8",
        "provider": "azure_ai",
        "input": 0.00000141,
        "output": 3.5e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 16384
    },
    {
        "model": "azure_ai/Llama-3.2-90B-Vision-Instruct",
        "provider": "azure_ai",
        "input": 0.00000204,
        "output": 0.00000204,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Meta-Llama-3.1-8B-Instruct",
        "provider": "azure_ai",
        "input": 3e-7,
        "output": 6.1e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Meta-Llama-3.1-70B-Instruct",
        "provider": "azure_ai",
        "input": 0.00000268,
        "output": 0.00000354,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Meta-Llama-3.1-405B-Instruct",
        "provider": "azure_ai",
        "input": 0.00000533,
        "output": 0.000016,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "azure_ai/Phi-4-mini-instruct",
        "provider": "azure_ai",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-4-multimodal-instruct",
        "provider": "azure_ai",
        "input": 8e-8,
        "output": 3.2e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3.5-mini-instruct",
        "provider": "azure_ai",
        "input": 1.3e-7,
        "output": 5.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3.5-vision-instruct",
        "provider": "azure_ai",
        "input": 1.3e-7,
        "output": 5.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3.5-MoE-instruct",
        "provider": "azure_ai",
        "input": 1.6e-7,
        "output": 6.4e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3-mini-128k-instruct",
        "provider": "azure_ai",
        "input": 1.3e-7,
        "output": 5.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3-small-128k-instruct",
        "provider": "azure_ai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/Phi-3-medium-128k-instruct",
        "provider": "azure_ai",
        "input": 1.7e-7,
        "output": 6.8e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "azure_ai/embed-v-4-0",
        "provider": "azure_ai",
        "input": 1.2e-7,
        "output": 0.0,
        "max_input_tokens": 128000,
        "max_output_tokens": null
    },
    {
        "model": "claude-instant-1",
        "provider": "anthropic",
        "input": 0.00000163,
        "output": 0.00000551,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "mistral/mistral-medium-latest",
        "provider": "mistral",
        "input": 4e-7,
        "output": 0.000002,
        "max_input_tokens": 131072,
        "max_output_tokens": 8191
    },
    {
        "model": "mistral/mistral-medium-2505",
        "provider": "mistral",
        "input": 4e-7,
        "output": 0.000002,
        "max_input_tokens": 131072,
        "max_output_tokens": 8191
    },
    {
        "model": "mistral/mistral-large-latest",
        "provider": "mistral",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/mistral-large-2411",
        "provider": "mistral",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/mistral-large-2407",
        "provider": "mistral",
        "input": 0.000003,
        "output": 0.000009,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/pixtral-large-latest",
        "provider": "mistral",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/pixtral-large-2411",
        "provider": "mistral",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/pixtral-12b-2409",
        "provider": "mistral",
        "input": 1.5e-7,
        "output": 1.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/open-mistral-nemo",
        "provider": "mistral",
        "input": 3e-7,
        "output": 3e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/open-mistral-nemo-2407",
        "provider": "mistral",
        "input": 3e-7,
        "output": 3e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "mistral/open-codestral-mamba",
        "provider": "mistral",
        "input": 2.5e-7,
        "output": 2.5e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "mistral/codestral-mamba-latest",
        "provider": "mistral",
        "input": 2.5e-7,
        "output": 2.5e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "mistral/devstral-small-2505",
        "provider": "mistral",
        "input": 1e-7,
        "output": 3e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "xai/grok-beta",
        "provider": "xai",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3",
        "provider": "xai",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-beta",
        "provider": "xai",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-fast-beta",
        "provider": "xai",
        "input": 0.000005,
        "output": 0.000025,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-fast-latest",
        "provider": "xai",
        "input": 0.000005,
        "output": 0.000025,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-mini-beta",
        "provider": "xai",
        "input": 3e-7,
        "output": 5e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-mini-fast-beta",
        "provider": "xai",
        "input": 6e-7,
        "output": 0.000004,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-3-mini-fast-latest",
        "provider": "xai",
        "input": 6e-7,
        "output": 0.000004,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-2-1212",
        "provider": "xai",
        "input": 0.000002,
        "output": 0.00001,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-2",
        "provider": "xai",
        "input": 0.000002,
        "output": 0.00001,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "xai/grok-2-latest",
        "provider": "xai",
        "input": 0.000002,
        "output": 0.00001,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "deepseek/deepseek-coder",
        "provider": "deepseek",
        "input": 1.4e-7,
        "output": 2.8e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "groq/deepseek-r1-distill-llama-70b",
        "provider": "groq",
        "input": 7.5e-7,
        "output": 9.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "groq/llama-3.3-70b-versatile",
        "provider": "groq",
        "input": 5.9e-7,
        "output": 7.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "groq/llama-3.1-8b-instant",
        "provider": "groq",
        "input": 5e-8,
        "output": 8e-8,
        "max_input_tokens": 128000,
        "max_output_tokens": 8192
    },
    {
        "model": "groq/meta-llama/llama-4-scout-17b-16e-instruct",
        "provider": "groq",
        "input": 1.1e-7,
        "output": 3.4e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 8192
    },
    {
        "model": "groq/meta-llama/llama-4-maverick-17b-128e-instruct",
        "provider": "groq",
        "input": 2e-7,
        "output": 6e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 8192
    },
    {
        "model": "groq/qwen-qwq-32b",
        "provider": "groq",
        "input": 2.9e-7,
        "output": 3.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "cerebras/llama3.1-8b",
        "provider": "cerebras",
        "input": 1e-7,
        "output": 1e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "cerebras/llama3.1-70b",
        "provider": "cerebras",
        "input": 6e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "cerebras/llama-3.3-70b",
        "provider": "cerebras",
        "input": 8.5e-7,
        "output": 0.0000012,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "cerebras/qwen-3-32b",
        "provider": "cerebras",
        "input": 4e-7,
        "output": 8e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "claude-instant-1.2",
        "provider": "anthropic",
        "input": 1.63e-7,
        "output": 5.51e-7,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "claude-2",
        "provider": "anthropic",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "claude-2.1",
        "provider": "anthropic",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 200000,
        "max_output_tokens": 8191
    },
    {
        "model": "claude-3-haiku-20240307",
        "provider": "anthropic",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "claude-3-5-haiku-20241022",
        "provider": "anthropic",
        "input": 8e-7,
        "output": 0.000004,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "claude-3-5-haiku-latest",
        "provider": "anthropic",
        "input": 0.000001,
        "output": 0.000005,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "claude-3-opus-latest",
        "provider": "anthropic",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "claude-3-opus-20240229",
        "provider": "anthropic",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "claude-3-sonnet-20240229",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "claude-3-5-sonnet-latest",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "claude-3-5-sonnet-20240620",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "claude-opus-4-20250514",
        "provider": "anthropic",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "claude-sonnet-4-20250514",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "claude-4-opus-20250514",
        "provider": "anthropic",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "claude-4-sonnet-20250514",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "claude-3-7-sonnet-latest",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 128000
    },
    {
        "model": "claude-3-7-sonnet-20250219",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 128000
    },
    {
        "model": "claude-3-5-sonnet-20241022",
        "provider": "anthropic",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.000005,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro-002",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.000005,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro-001",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.000005,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro-preview-0514",
        "provider": "vertex_ai-language-models",
        "input": 7.8125e-8,
        "output": 3.125e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro-preview-0215",
        "provider": "vertex_ai-language-models",
        "input": 7.8125e-8,
        "output": 3.125e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-pro-preview-0409",
        "provider": "vertex_ai-language-models",
        "input": 7.8125e-8,
        "output": 3.125e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-flash",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-flash-exp-0827",
        "provider": "vertex_ai-language-models",
        "input": 4.688e-9,
        "output": 4.6875e-9,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-flash-002",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-flash-001",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-1.5-flash-preview-0514",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 4.6875e-9,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-pro-experimental",
        "provider": "vertex_ai-language-models",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-flash-experimental",
        "provider": "vertex_ai-language-models",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.5-pro-exp-03-25",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.0-pro-exp-02-05",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-exp",
        "provider": "vertex_ai-language-models",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-001",
        "provider": "vertex_ai-language-models",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-thinking-exp",
        "provider": "vertex_ai-language-models",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-thinking-exp-01-21",
        "provider": "vertex_ai-language-models",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65536
    },
    {
        "model": "gemini/gemini-2.5-pro-exp-03-25",
        "provider": "gemini",
        "input": 0.0,
        "output": 0.0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-flash-preview-tts",
        "provider": "gemini",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-flash-preview-05-20",
        "provider": "gemini",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-flash-preview-04-17",
        "provider": "gemini",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.5-flash-preview-05-20",
        "provider": "vertex_ai-language-models",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.5-flash-preview-04-17",
        "provider": "vertex_ai-language-models",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.0-flash",
        "provider": "vertex_ai-language-models",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-lite",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.0-flash-lite-001",
        "provider": "vertex_ai-language-models",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.5-pro-preview-06-05",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.5-pro-preview-05-06",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.5-pro-preview-03-25",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini-2.0-flash-preview-image-generation",
        "provider": "vertex_ai-language-models",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini-2.5-pro-preview-tts",
        "provider": "vertex_ai-language-models",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.0-pro-exp-02-05",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash-preview-image-generation",
        "provider": "gemini",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash",
        "provider": "gemini",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash-lite",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash-001",
        "provider": "gemini",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.5-pro-preview-tts",
        "provider": "gemini",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-pro-preview-06-05",
        "provider": "gemini",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-pro-preview-05-06",
        "provider": "gemini",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.5-pro-preview-03-25",
        "provider": "gemini",
        "input": 0.00000125,
        "output": 0.00001,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65535
    },
    {
        "model": "gemini/gemini-2.0-flash-exp",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash-lite-preview-02-05",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-2.0-flash-thinking-exp",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65536
    },
    {
        "model": "gemini/gemini-2.0-flash-thinking-exp-01-21",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 65536
    },
    {
        "model": "gemini/gemma-3-27b-it",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 131072,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-sonnet",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/claude-3-sonnet@20240229",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/claude-3-5-sonnet",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-5-sonnet@20240620",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-5-sonnet-v2",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-5-sonnet-v2@20241022",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-7-sonnet@20250219",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-opus-4",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "vertex_ai/claude-opus-4@20250514",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "vertex_ai/claude-sonnet-4",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "vertex_ai/claude-sonnet-4@20250514",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "vertex_ai/claude-3-haiku",
        "provider": "vertex_ai-anthropic_models",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/claude-3-haiku@20240307",
        "provider": "vertex_ai-anthropic_models",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/claude-3-5-haiku",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000001,
        "output": 0.000005,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-5-haiku@20241022",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000001,
        "output": 0.000005,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "vertex_ai/claude-3-opus",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/claude-3-opus@20240229",
        "provider": "vertex_ai-anthropic_models",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "vertex_ai/meta/llama-4-scout-17b-16e-instruct-maas",
        "provider": "vertex_ai-llama_models",
        "input": 2.5e-7,
        "output": 7e-7,
        "max_input_tokens": 10000000.0,
        "max_output_tokens": 10000000.0
    },
    {
        "model": "vertex_ai/meta/llama-4-scout-17b-128e-instruct-maas",
        "provider": "vertex_ai-llama_models",
        "input": 2.5e-7,
        "output": 7e-7,
        "max_input_tokens": 10000000.0,
        "max_output_tokens": 10000000.0
    },
    {
        "model": "vertex_ai/meta/llama-4-maverick-17b-128e-instruct-maas",
        "provider": "vertex_ai-llama_models",
        "input": 3.5e-7,
        "output": 0.00000115,
        "max_input_tokens": 1000000.0,
        "max_output_tokens": 1000000.0
    },
    {
        "model": "vertex_ai/meta/llama-4-maverick-17b-16e-instruct-maas",
        "provider": "vertex_ai-llama_models",
        "input": 3.5e-7,
        "output": 0.00000115,
        "max_input_tokens": 1000000.0,
        "max_output_tokens": 1000000.0
    },
    {
        "model": "vertex_ai/meta/llama-3.2-90b-vision-instruct-maas",
        "provider": "vertex_ai-llama_models",
        "input": 0.0,
        "output": 0.0,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "vertex_ai/mistral-large@latest",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 8191
    },
    {
        "model": "vertex_ai/mistral-large@2411-001",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 8191
    },
    {
        "model": "vertex_ai/mistral-large-2411",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 8191
    },
    {
        "model": "vertex_ai/mistral-large@2407",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000002,
        "output": 0.000006,
        "max_input_tokens": 128000,
        "max_output_tokens": 8191
    },
    {
        "model": "vertex_ai/mistral-nemo@latest",
        "provider": "vertex_ai-mistral_models",
        "input": 1.5e-7,
        "output": 1.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "vertex_ai/mistral-small-2503",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000001,
        "output": 0.000003,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "vertex_ai/jamba-1.5-mini@001",
        "provider": "vertex_ai-ai21_models",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "vertex_ai/jamba-1.5-large@001",
        "provider": "vertex_ai-ai21_models",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "vertex_ai/jamba-1.5",
        "provider": "vertex_ai-ai21_models",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "vertex_ai/jamba-1.5-mini",
        "provider": "vertex_ai-ai21_models",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "vertex_ai/jamba-1.5-large",
        "provider": "vertex_ai-ai21_models",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "vertex_ai/mistral-nemo@2407",
        "provider": "vertex_ai-mistral_models",
        "input": 0.000003,
        "output": 0.000003,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "vertex_ai/codestral@latest",
        "provider": "vertex_ai-mistral_models",
        "input": 2e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "vertex_ai/codestral@2405",
        "provider": "vertex_ai-mistral_models",
        "input": 2e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "vertex_ai/codestral-2501",
        "provider": "vertex_ai-mistral_models",
        "input": 2e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "gemini/gemini-1.5-flash-002",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-001",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-latest",
        "provider": "gemini",
        "input": 7.5e-8,
        "output": 3e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-8b",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-8b-exp-0924",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-exp-1114",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-exp-1206",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-exp-0827",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-flash-8b-exp-0827",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro",
        "provider": "gemini",
        "input": 0.0000035,
        "output": 0.0000105,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro-002",
        "provider": "gemini",
        "input": 0.0000035,
        "output": 0.0000105,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro-001",
        "provider": "gemini",
        "input": 0.0000035,
        "output": 0.0000105,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro-exp-0801",
        "provider": "gemini",
        "input": 0.0000035,
        "output": 0.0000105,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro-exp-0827",
        "provider": "gemini",
        "input": 0,
        "output": 0,
        "max_input_tokens": 2097152,
        "max_output_tokens": 8192
    },
    {
        "model": "gemini/gemini-1.5-pro-latest",
        "provider": "gemini",
        "input": 0.0000035,
        "output": 0.00000105,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "command-a-03-2025",
        "provider": "cohere_chat",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 256000,
        "max_output_tokens": 8000
    },
    {
        "model": "command-r",
        "provider": "cohere_chat",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "command-r-08-2024",
        "provider": "cohere_chat",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "command-r7b-12-2024",
        "provider": "cohere_chat",
        "input": 1.5e-7,
        "output": 3.75e-8,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "command-r-plus",
        "provider": "cohere_chat",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "command-r-plus-08-2024",
        "provider": "cohere_chat",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "openrouter/google/gemini-pro-1.5",
        "provider": "openrouter",
        "input": 0.0000025,
        "output": 0.0000075,
        "max_input_tokens": 1000000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/google/gemini-2.0-flash-001",
        "provider": "openrouter",
        "input": 1e-7,
        "output": 4e-7,
        "max_input_tokens": 1048576,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/anthropic/claude-3-haiku-20240307",
        "provider": "openrouter",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "openrouter/anthropic/claude-3-5-haiku-20241022",
        "provider": "openrouter",
        "input": 0.000001,
        "output": 0.000005,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/anthropic/claude-3.5-sonnet",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/anthropic/claude-3.5-sonnet:beta",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/anthropic/claude-3.7-sonnet",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/anthropic/claude-3.7-sonnet:beta",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "openrouter/openai/o1",
        "provider": "openrouter",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 200000,
        "max_output_tokens": 100000
    },
    {
        "model": "openrouter/openai/o1-mini",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000012,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "openrouter/openai/o1-mini-2024-09-12",
        "provider": "openrouter",
        "input": 0.000003,
        "output": 0.000012,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "openrouter/openai/o1-preview",
        "provider": "openrouter",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "openrouter/openai/o1-preview-2024-09-12",
        "provider": "openrouter",
        "input": 0.000015,
        "output": 0.00006,
        "max_input_tokens": 128000,
        "max_output_tokens": 32768
    },
    {
        "model": "openrouter/openai/o3-mini",
        "provider": "openrouter",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "openrouter/openai/o3-mini-high",
        "provider": "openrouter",
        "input": 0.0000011,
        "output": 0.0000044,
        "max_input_tokens": 128000,
        "max_output_tokens": 65536
    },
    {
        "model": "openrouter/openai/gpt-4o",
        "provider": "openrouter",
        "input": 0.0000025,
        "output": 0.00001,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "openrouter/openai/gpt-4o-2024-05-13",
        "provider": "openrouter",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "openrouter/anthropic/claude-3-opus",
        "provider": "openrouter",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "jamba-1.5-mini@001",
        "provider": "ai21",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-1.5-large@001",
        "provider": "ai21",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-1.5",
        "provider": "ai21",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-1.5-mini",
        "provider": "ai21",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-1.5-large",
        "provider": "ai21",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-large-1.6",
        "provider": "ai21",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "jamba-mini-1.6",
        "provider": "ai21",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "ai21.jamba-1-5-large-v1:0",
        "provider": "bedrock",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "ai21.jamba-1-5-mini-v1:0",
        "provider": "bedrock",
        "input": 2e-7,
        "output": 4e-7,
        "max_input_tokens": 256000,
        "max_output_tokens": 256000
    },
    {
        "model": "mistral.mistral-large-2407-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000009,
        "max_input_tokens": 128000,
        "max_output_tokens": 8191
    },
    {
        "model": "amazon.nova-micro-v1:0",
        "provider": "bedrock_converse",
        "input": 3.5e-8,
        "output": 1.4e-7,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "us.amazon.nova-micro-v1:0",
        "provider": "bedrock_converse",
        "input": 3.5e-8,
        "output": 1.4e-7,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "eu.amazon.nova-micro-v1:0",
        "provider": "bedrock_converse",
        "input": 4.6e-8,
        "output": 1.84e-7,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "amazon.nova-lite-v1:0",
        "provider": "bedrock_converse",
        "input": 6e-8,
        "output": 2.4e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 10000
    },
    {
        "model": "us.amazon.nova-lite-v1:0",
        "provider": "bedrock_converse",
        "input": 6e-8,
        "output": 2.4e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 10000
    },
    {
        "model": "eu.amazon.nova-lite-v1:0",
        "provider": "bedrock_converse",
        "input": 7.8e-8,
        "output": 3.12e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 10000
    },
    {
        "model": "amazon.nova-pro-v1:0",
        "provider": "bedrock_converse",
        "input": 8e-7,
        "output": 0.0000032,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "us.amazon.nova-pro-v1:0",
        "provider": "bedrock_converse",
        "input": 8e-7,
        "output": 0.0000032,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "eu.amazon.nova-pro-v1:0",
        "provider": "bedrock_converse",
        "input": 0.00000105,
        "output": 0.0000042,
        "max_input_tokens": 300000,
        "max_output_tokens": 10000
    },
    {
        "model": "us.amazon.nova-premier-v1:0",
        "provider": "bedrock_converse",
        "input": 0.0000025,
        "output": 0.0000125,
        "max_input_tokens": 1000000,
        "max_output_tokens": 10000
    },
    {
        "model": "anthropic.claude-3-sonnet-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "bedrock/invoke/anthropic.claude-3-5-sonnet-20240620-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "anthropic.claude-3-5-sonnet-20240620-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "anthropic.claude-opus-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "anthropic.claude-sonnet-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "anthropic.claude-3-7-sonnet-20250219-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "anthropic.claude-3-haiku-20240307-v1:0",
        "provider": "bedrock",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
        "provider": "bedrock",
        "input": 8e-7,
        "output": 0.000004,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "anthropic.claude-3-opus-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.anthropic.claude-3-sonnet-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.anthropic.claude-3-5-sonnet-20240620-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "us.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "us.anthropic.claude-opus-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "us.anthropic.claude-sonnet-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "us.anthropic.claude-3-haiku-20240307-v1:0",
        "provider": "bedrock",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.anthropic.claude-3-5-haiku-20241022-v1:0",
        "provider": "bedrock",
        "input": 8e-7,
        "output": 0.000004,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "us.anthropic.claude-3-opus-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.anthropic.claude-3-sonnet-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.anthropic.claude-3-5-sonnet-20240620-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.anthropic.claude-3-5-sonnet-20241022-v2:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "eu.anthropic.claude-3-7-sonnet-20250219-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "eu.anthropic.claude-3-haiku-20240307-v1:0",
        "provider": "bedrock",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.anthropic.claude-opus-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 32000
    },
    {
        "model": "eu.anthropic.claude-sonnet-4-20250514-v1:0",
        "provider": "bedrock_converse",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 64000
    },
    {
        "model": "eu.anthropic.claude-3-5-haiku-20241022-v1:0",
        "provider": "bedrock",
        "input": 2.5e-7,
        "output": 0.00000125,
        "max_input_tokens": 200000,
        "max_output_tokens": 8192
    },
    {
        "model": "eu.anthropic.claude-3-opus-20240229-v1:0",
        "provider": "bedrock",
        "input": 0.000015,
        "output": 0.000075,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "anthropic.claude-v1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-east-1/anthropic.claude-v1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-west-2/anthropic.claude-v1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/ap-northeast-1/anthropic.claude-v1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/eu-central-1/anthropic.claude-v1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "anthropic.claude-v2",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-east-1/anthropic.claude-v2",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-west-2/anthropic.claude-v2",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/ap-northeast-1/anthropic.claude-v2",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/eu-central-1/anthropic.claude-v2",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "anthropic.claude-v2:1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-east-1/anthropic.claude-v2:1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-west-2/anthropic.claude-v2:1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/ap-northeast-1/anthropic.claude-v2:1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/eu-central-1/anthropic.claude-v2:1",
        "provider": "bedrock",
        "input": 0.000008,
        "output": 0.000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "anthropic.claude-instant-v1",
        "provider": "bedrock",
        "input": 8e-7,
        "output": 0.0000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-east-1/anthropic.claude-instant-v1",
        "provider": "bedrock",
        "input": 8e-7,
        "output": 0.0000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/us-west-2/anthropic.claude-instant-v1",
        "provider": "bedrock",
        "input": 8e-7,
        "output": 0.0000024,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/ap-northeast-1/anthropic.claude-instant-v1",
        "provider": "bedrock",
        "input": 0.00000223,
        "output": 0.00000755,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "bedrock/eu-central-1/anthropic.claude-instant-v1",
        "provider": "bedrock",
        "input": 0.00000248,
        "output": 0.00000838,
        "max_input_tokens": 100000,
        "max_output_tokens": 8191
    },
    {
        "model": "cohere.command-r-plus-v1:0",
        "provider": "bedrock",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "cohere.command-r-v1:0",
        "provider": "bedrock",
        "input": 5e-7,
        "output": 0.0000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.deepseek.r1-v1:0",
        "provider": "bedrock_converse",
        "input": 0.00000135,
        "output": 0.0000054,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-3-70b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 7.2e-7,
        "output": 7.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-1-8b-instruct-v1:0",
        "provider": "bedrock",
        "input": 2.2e-7,
        "output": 2.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "us.meta.llama3-1-8b-instruct-v1:0",
        "provider": "bedrock",
        "input": 2.2e-7,
        "output": 2.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "meta.llama3-1-70b-instruct-v1:0",
        "provider": "bedrock",
        "input": 9.9e-7,
        "output": 9.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "us.meta.llama3-1-70b-instruct-v1:0",
        "provider": "bedrock",
        "input": 9.9e-7,
        "output": 9.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 2048
    },
    {
        "model": "meta.llama3-1-405b-instruct-v1:0",
        "provider": "bedrock",
        "input": 0.00000532,
        "output": 0.000016,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-1-405b-instruct-v1:0",
        "provider": "bedrock",
        "input": 0.00000532,
        "output": 0.000016,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-2-1b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1e-7,
        "output": 1e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-2-1b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1e-7,
        "output": 1e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.meta.llama3-2-1b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1.3e-7,
        "output": 1.3e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-2-3b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1.5e-7,
        "output": 1.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-2-3b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1.5e-7,
        "output": 1.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "eu.meta.llama3-2-3b-instruct-v1:0",
        "provider": "bedrock",
        "input": 1.9e-7,
        "output": 1.9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-2-11b-instruct-v1:0",
        "provider": "bedrock",
        "input": 3.5e-7,
        "output": 3.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-2-11b-instruct-v1:0",
        "provider": "bedrock",
        "input": 3.5e-7,
        "output": 3.5e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama3-2-90b-instruct-v1:0",
        "provider": "bedrock",
        "input": 0.000002,
        "output": 0.000002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-2-90b-instruct-v1:0",
        "provider": "bedrock",
        "input": 0.000002,
        "output": 0.000002,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama3-3-70b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 7.2e-7,
        "output": 7.2e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama4-maverick-17b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 2.4e-7,
        "output": 9.7e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama4-maverick-17b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 2.4e-7,
        "output": 9.7e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "meta.llama4-scout-17b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 1.7e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "us.meta.llama4-scout-17b-instruct-v1:0",
        "provider": "bedrock_converse",
        "input": 1.7e-7,
        "output": 6.6e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 4096
    },
    {
        "model": "deepinfra/01-ai/Yi-6B-200K",
        "provider": "deepinfra",
        "input": 1.3e-7,
        "output": 1.3e-7,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "deepinfra/01-ai/Yi-34B-200K",
        "provider": "deepinfra",
        "input": 6e-7,
        "output": 6e-7,
        "max_input_tokens": 200000,
        "max_output_tokens": 4096
    },
    {
        "model": "perplexity/llama-3.1-70b-instruct",
        "provider": "perplexity",
        "input": 0.000001,
        "output": 0.000001,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "perplexity/llama-3.1-8b-instruct",
        "provider": "perplexity",
        "input": 2e-7,
        "output": 2e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "perplexity/llama-3.1-sonar-huge-128k-online",
        "provider": "perplexity",
        "input": 0.000005,
        "output": 0.000005,
        "max_input_tokens": 127072,
        "max_output_tokens": 127072
    },
    {
        "model": "perplexity/llama-3.1-sonar-large-128k-online",
        "provider": "perplexity",
        "input": 0.000001,
        "output": 0.000001,
        "max_input_tokens": 127072,
        "max_output_tokens": 127072
    },
    {
        "model": "perplexity/llama-3.1-sonar-large-128k-chat",
        "provider": "perplexity",
        "input": 0.000001,
        "output": 0.000001,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "perplexity/llama-3.1-sonar-small-128k-chat",
        "provider": "perplexity",
        "input": 2e-7,
        "output": 2e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "perplexity/llama-3.1-sonar-small-128k-online",
        "provider": "perplexity",
        "input": 2e-7,
        "output": 2e-7,
        "max_input_tokens": 127072,
        "max_output_tokens": 127072
    },
    {
        "model": "perplexity/sonar",
        "provider": "perplexity",
        "input": 0.000001,
        "output": 0.000001,
        "max_input_tokens": 128000,
        "max_output_tokens": null
    },
    {
        "model": "perplexity/sonar-pro",
        "provider": "perplexity",
        "input": 0.000003,
        "output": 0.000015,
        "max_input_tokens": 200000,
        "max_output_tokens": 8000
    },
    {
        "model": "perplexity/sonar-reasoning",
        "provider": "perplexity",
        "input": 0.000001,
        "output": 0.000005,
        "max_input_tokens": 128000,
        "max_output_tokens": null
    },
    {
        "model": "perplexity/sonar-reasoning-pro",
        "provider": "perplexity",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 128000,
        "max_output_tokens": null
    },
    {
        "model": "perplexity/sonar-deep-research",
        "provider": "perplexity",
        "input": 0.000002,
        "output": 0.000008,
        "max_input_tokens": 128000,
        "max_output_tokens": null
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/deepseek-v3",
        "provider": "fireworks_ai",
        "input": 9e-7,
        "output": 9e-7,
        "max_input_tokens": 128000,
        "max_output_tokens": 8192
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1",
        "provider": "fireworks_ai",
        "input": 0.000003,
        "output": 0.000008,
        "max_input_tokens": 128000,
        "max_output_tokens": 20480
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-basic",
        "provider": "fireworks_ai",
        "input": 5.5e-7,
        "output": 0.00000219,
        "max_input_tokens": 128000,
        "max_output_tokens": 20480
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/deepseek-r1-0528",
        "provider": "fireworks_ai",
        "input": 0.000003,
        "output": 0.000008,
        "max_input_tokens": 160000,
        "max_output_tokens": 160000
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/llama-v3p1-405b-instruct",
        "provider": "fireworks_ai",
        "input": 0.000003,
        "output": 0.000003,
        "max_input_tokens": 128000,
        "max_output_tokens": 16384
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/llama4-maverick-instruct-basic",
        "provider": "fireworks_ai",
        "input": 2.2e-7,
        "output": 8.8e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "fireworks_ai/accounts/fireworks/models/llama4-scout-instruct-basic",
        "provider": "fireworks_ai",
        "input": 1.5e-7,
        "output": 6e-7,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "databricks/databricks-claude-3-7-sonnet",
        "provider": "databricks",
        "input": 0.0000025,
        "output": 0.000017857,
        "max_input_tokens": 200000,
        "max_output_tokens": 128000
    },
    {
        "model": "databricks/databricks-meta-llama-3-1-405b-instruct",
        "provider": "databricks",
        "input": 0.000005,
        "output": 0.00001500002,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "databricks/databricks-meta-llama-3-1-70b-instruct",
        "provider": "databricks",
        "input": 0.00000100002,
        "output": 0.00000299999,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "databricks/databricks-meta-llama-3-3-70b-instruct",
        "provider": "databricks",
        "input": 0.00000100002,
        "output": 0.00000299999,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "databricks/databricks-llama-4-maverick",
        "provider": "databricks",
        "input": 0.000005,
        "output": 0.000015,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "databricks/databricks-meta-llama-3-70b-instruct",
        "provider": "databricks",
        "input": 0.00000100002,
        "output": 0.00000299999,
        "max_input_tokens": 128000,
        "max_output_tokens": 128000
    },
    {
        "model": "sambanova/Llama-4-Maverick-17B-128E-Instruct",
        "provider": "sambanova",
        "input": 6.3e-7,
        "output": 0.0000018,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "sambanova/Meta-Llama-3.3-70B-Instruct",
        "provider": "sambanova",
        "input": 6e-7,
        "output": 0.0000012,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    },
    {
        "model": "sambanova/DeepSeek-R1-Distill-Llama-70B",
        "provider": "sambanova",
        "input": 7e-7,
        "output": 0.0000014,
        "max_input_tokens": 131072,
        "max_output_tokens": 131072
    }
]
